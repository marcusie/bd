分工 : 
- 刘凯杰：4.2-hive数据分析，5.1-flink流数据实时分析，5-3流数据实时分析与前端数据实时可视化
- 刘照辉：3-数据采集与数据存储，4.1-mapreduce数据分析，7-数据可视化
- 李平颐：4.3-spark-sql数据分析，5.2-spark流数据实时分析，6-高级数据分析

#  招聘信息大数据分析

## 1.项目概述

### 1.1 项目背景

### 1.2 项目功能

### 1.3 关键技术

### 1.4 运行环境

- Linux：Ubuntu 16.04
- Hadoop：2.7.1
- Eclipse：3.8
- Hive：1.2.1
- Flink：1.11
- FastAPI：1.2.1
- Echart：4

## 2.系统架构设计
### 2.1 系统组成

本系统基于伪分布式架构部署，所有组件运行在同一物理节点，逻辑上模拟分布式环境。核心组件包括：
- Hadoop 生态
    - HDFS：分布式文件系统，提供高可靠存储（NameNode + DataNode）
    - YARN：资源调度与管理框架（ResourceManager + NodeManager）
    - MapReduce：分布式计算引擎（JobHistoryServer）
- HBase：基于 HDFS 的分布式列式数据库（HMaster + RegionServer + ZooKeeper）
- Spark：内存计算引擎，支持批处理与流处理（Standalone 模式或 YARN 模式）
- Flink：流批一体计算引擎（Standalone Session Cluster 模式）
- Hive：数据仓库工具，提供 SQL 查询能力（Metastore + HiveServer2）
- 辅助组件
    - ZooKeeper：协调服务（HBase 依赖）
    - MySQL：Hive 元数据存储（替代默认 Derby）

### 2.2 系统协作方式

组件间通过分层协作与服务调用实现数据流转：
1. 存储层
    - HDFS 作为统一存储底座，HBase/Hive/Spark/Flink 均直接读写 HDFS。
    - HBase 依赖 HDFS 存储 HFile，Hive 表数据默认存于 HDFS。
2. 计算层
    - 批处理：
        - MapReduce/Spark/Flink 通过 HDFS 客户端读取数据，计算结果写回 HDFS。
        - Hive 将 SQL 转译为 MapReduce/Spark 作业（需配置 `hive.execution.engine=spark`）。
    - 流处理：
        - Flink 从 Kafka/File 等源读取实时数据（伪分布下可模拟），结果写入 HDFS。
3. 资源调度
    - YARN 统一管理计算资源：Spark/Flink/Hive（MapReduce 模式）均向 YARN 申请容器（Container）。
    - HBase 自主管理 RegionServer 资源，不依赖 YARN。
4. 元数据管理
    - Hive 元数据存储于 MySQL，HBase 依赖 ZooKeeper 维护集群状态。

### 2.3 系统网络拓扑

伪分布式系统采用单节点环回网络，所有服务通过 `localhost`（127.0.0.1）通信：
- 所有组件通过 本地环回地址（127.0.0.1） 通信，端口隔离（如 HDFS 9000、YARN 8088、HBase 16010 等）。
- 无跨节点网络延迟，但资源竞争需通过 YARN 限制（如 CPU/内存配额）。
- 外部客户端通过 `localhost:端口` 访问服务（如 HiveServer2 的 10000 端口）。

![[Pasted image 20251106113817.png]]

## 3.数据采集与数据存储
### 3.1 数据采集

1. 爬取工具：八爪鱼
2. 数据来源：Boss直聘（https://www.zhipin.com/）
3. 原始数据量： 13198
4. 数据字段：城市，关键词。职位，职位详情链接，地区，薪资范围，经验要求，学历要求，公司名称，公司详情链接，公司类型  融资情况，公司规模，标签1，标签2 ，标签3，标签4，标签5，福利待遇

- 原始数据集展示：
![[image-20241108161611427.png]]


### 3.2 数据预处理

#### 3.2.1 去除无用字段、重复行，添加id

```python
import pandas as pd
df = pd.read_csv('recruit-0.csv')
df.drop(columns=['职位详情链接', '公司详情链接'], inplace=True)
df.drop_duplicates(inplace=True)
df.insert(0, 'id', range(1, len(df) + 1))
df.to_csv('recruit-1.csv', index=False)
print("数据处理完成，结果已保存到 'recruit-1.csv'")
```

![[image-20241109174649976.png]]

#### 3.2.2 数据格式化处理

结果：
![[image-20241109175805047.png]]

步骤：

1. 薪资-取中间值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-1.csv', sep=',')
def extract_salary_midpoint(salary_str):
    match = re.match(r'(\d+)-(\d+)K(?:·(\d+)薪)?', salary_str)
    if match:
        lower, upper = int(match.group(1)), int(match.group(2))
        midpoint = (lower + upper) / 2
        return midpoint
    return None
df['薪资中值'] = df['薪资范围'].apply(extract_salary_midpoint)
df = df.drop(columns=['薪资范围'])
df = df.dropna(subset=['薪资中值'])
df.to_csv('recruit-2.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-2.csv'")
```

2. 工作经验值-取最低值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-2.csv', sep=',')
def extract_min_experience(experience_str):
    match = re.match(r'(\d+)-?(\d*)年', experience_str)
    if match:
        min_exp = int(match.group(1))
        return min_exp
    elif experience_str == '1年以内':
        return 0
    elif experience_str == '经验不限':
        return 0
    return None
df['最低经验要求'] = df['经验要求'].apply(extract_min_experience)
df = df.drop(columns=['经验要求'])
df = df.dropna(subset=['最低经验要求'])
df.to_csv('recruit-3.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-3.csv'")
```

3. 公司规模与融资情况处理：合并，规模取中间值
```python
import pandas as pd
import csv
import re
input_file = 'recruit-3.csv'
output_file = 'recruit-4.csv'
def extract_scale(financing_info):
    # 使用正则表达式匹配公司规模
    match = re.search(r'(\d+-\d+|\d+)人|(\d+)以上', financing_info)
    if match:
        return match.group(1) or match.group(2)
    return None
def calculate_midpoint(scale):
    if '-' in scale:
        lower, upper = map(int, scale.split('-'))
        return (lower + upper) // 2
    elif scale.isdigit():
        return int(scale)
    elif '以上' in scale:
        return int(scale.replace('以上', ''))
    else:
        return None
try:
    with open(input_file, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        headers = [field for field in reader.fieldnames if field not in ['公司规模', '融资情况']]
        headers.append('公司规模中值')
        rows = list(reader)
except FileNotFoundError:
    print(f"文件 {input_file} 未找到，请检查文件路径。")
    exit(1)
processed_rows = []
for row in rows:
    scale = row['公司规模'] or extract_scale(row['融资情况'])  # 如果公司规模为空，则从融资情况中提取
    if scale:
        scale = scale.replace('人', '')
        midpoint = calculate_midpoint(scale)
        if midpoint is not None:
            new_row = {key: value for key, value in row.items() if key not in ['公司规模', '融资情况']}
            new_row['公司规模中值'] = midpoint
            processed_rows.append(new_row)
with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.DictWriter(file, fieldnames=headers)
    writer.writeheader()
    writer.writerows(processed_rows)
print(f"处理完成，结果已保存到 {output_file}")
```

#### 3.2.3 多值列表化处理

结果：
![[image-20241109180042964.png]]

步骤：

1. 合并标签
```python
import pandas as pd
df = pd.read_csv('recruit-4.csv', sep=',')
# 定义一个函数来将多列合并为一个列表
def merge_to_list(row):
    return [str(item) for item in row if pd.notnull(item)]
df['标签'] = df[['标签1', '标签2', '标签3', '标签4', '标签5']].apply(merge_to_list, axis=1)
df = df.drop(columns=['标签1', '标签2', '标签3', '标签4', '标签5'])
df.to_csv('recruit-5.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-5.csv'")
```

2. 处理福利待遇转列表
```python
import pandas as pd
import re
df = pd.read_csv('recruit-5.csv', sep=',')
def split_welfare(welfare_str):
    if pd.isnull(welfare_str):
        return []
    return [item.strip() for item in re.split(r'，', welfare_str)]
df['福利待遇列表'] = df['福利待遇'].apply(split_welfare)
df = df.drop(columns=['福利待遇'])
df.to_csv('recruit-6.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-6.csv'")
```

#### 3.2.4 最终结果

![[image-20241109180206673.png]]

### 3.3 数据上传与数据存储

#### 3.3.1 启动并上传数据至Hadoop

启动hadoop
```shell
start-dfs.sh
jps
```
![[image-20250625162057427.png]]
启动成功

数据上传至hdfs
1. 在hdfs中创建存储数据的文件夹
2. 把recruit.csv上传到hdfs的/data/中
3. 查看hdfs中的/data/recruit.csv前十条记录，说明上传成功
```shell
hdfs dfs -mkdir -p /data/recruit
hdfs dfs -put ~/data/recruit.csv /data/recruit
hdfs dfs -cat /data/recruit/recruit.csv | head
```
![[image-20250625163129785.png]]

### 3.4 数据库操作
#### 3.4.1 数据导入

- 启动hbase
```shell
start-hbase.sh
hbase thrift start
hbase shell
count 'recruit'
scan 'recruit', {LIMIT => 5}
```

- 导入数据
```python
python importToHbase.py

import happybase
import csv
def connect_to_hbase():
    connection = happybase.Connection('localhost')
    return connection
def create_table(connection, table_name):
    if bytes(table_name, encoding='utf-8') in connection.tables():
        print(f"Table {table_name} already exists.")
    else:
        families = {
            'info': dict(),  # 列族
        }
        connection.create_table(table_name, families)
        print(f"Created table {table_name}.")
def insert_data_into_hbase(connection, table_name, data):
    table = connection.table(table_name)
    record_count = 0  # 计数器初始化
    with table.batch() as b:
        for row in data:
            row_key = f"{row[0]}".encode('utf-8')
            row_data = {
                'info:city': row[1].encode('utf-8'),
                'info:keyword': row[2].encode('utf-8'),
                'info:position': row[3].encode('utf-8'),
                'info:area': row[4].encode('utf-8'),
                'info:education_requirement': row[5].encode('utf-8'),
                'info:company_name': row[6].encode('utf-8'),
                'info:company_type': row[7].encode('utf-8'),
                'info:salary_median': str(row[8]).encode('utf-8'),  # 将浮点数转换为字符串再编码
                'info:experience_min': str(row[9]).encode('utf-8'),  # 将整数转换为字符串再编码
                'info:company_size_median': str(row[10]).encode('utf-8'),  # 将整数转换为字符串再编码
                'info:tags': row[11].encode('utf-8'),
                'info:benefits': row[12].encode('utf-8')
            }
            b.put(row_key, row_data)
            record_count += 1  # 每次插入后递增计数器
    print(f"Inserted {record_count} records into the table {table_name}.")

def read_csv_and_prepare_data(file_path):
    data = []
    with open(file_path, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        next(reader)  # 跳过标题行
        for row in reader:
            row[8] = float(row[8])  # 薪资中值
            row[9] = int(float(row[9]))  # 最低经验要求
            row[10] = int(float(row[10]))  # 公司规模中值
            data.append(row)
    return data
if __name__ == '__main__':
    conn = connect_to_hbase()
    create_table(conn, 'recruit')
    data = read_csv_and_prepare_data('recruit.csv')
    insert_data_into_hbase(conn, 'recruit', data)
    conn.close()
```

- 验证
```sql
count 'recruit'
```

#### 3.4.2 数据库操作

- 操作菜单         
```python
python hbaseOperate.py

import happybase
import sys

def connect_to_hbase():
    try:
        connection = happybase.Connection('localhost')
        return connection
    except Exception as e:
        print(f"Error connecting to HBase: {e}")
        sys.exit(1)

def create_table(connection, table_name):
    try:
        if bytes(table_name, encoding='utf-8') not in connection.tables():
            families = {'info': dict()}
            connection.create_table(table_name, families)
            print(f"Table {table_name} created.")
        else:
            print(f"Table {table_name} already exists.")
    except Exception as e:
        print(f"Error creating table: {e}")

def fuzzy_query(connection, table_name, column, pattern):
    try:
        table = connection.table(table_name)
        # 使用PrefixFilter实现模糊查询
        rows = table.scan(filter=f"PrefixFilter('{pattern}')")
        for key, data in rows:
            print(f"Row Key: {key.decode('utf-8')}, Column: {column}, Value: {data.get(column.encode('utf-8')).decode('utf-8')}")
    except Exception as e:
        print(f"Error performing fuzzy query: {e}")

def main_menu():
    print("\nHBase Operations Menu:")
    print("1. Insert Data")
    print("2. Update Data")
    print("3. Delete Data")
    print("4. Query by ID")
    print("5. Query by Field")
    print("6. Fuzzy Query")
    print("0. Exit")
    choice = input("Choose an operation (0-6): ")
    return choice

def get_conditions():
    conditions = {}
    while True:
        field = input("Enter field name (without 'info:'), or press Enter to finish: ")
        if not field:
            break
        value = input(f"Enter value to match for field '{field}': ")
        conditions[field] = value
    return conditions

def get_update_fields():
    fields = {}
    while True:
        field = input("Enter field name to update (without 'info:'), or press Enter to finish: ")
        if not field:
            break
        value = input(f"Enter new value for field '{field}': ")
        fields[f'info:{field}'] = value
    return fields

def main():
    conn = connect_to_hbase()
    create_table(conn, 'recruit')

    while True:
        choice = main_menu()
        if choice == '1':
            row_key = input("Enter Row Key: ")
            data_dict = {}
            for i in ['city', 'keyword', 'position', 'area', 'education_requirement', 'company_name', 'company_type', 'salary_median', 'experience_min', 'company_size_median', 'tags', 'benefits']:
                data_dict[f'info:{i}'] = input(f"Enter {i}: ")
            insert_data(conn, 'recruit', row_key, data_dict)
        elif choice == '2':
            row_key = input("Enter Row Key to update: ")
            fields = get_update_fields()
            update_data(conn, 'recruit', row_key, fields)
        elif choice == '3':
            row_key = input("Enter Row Key to delete: ")
            delete_data(conn, 'recruit', row_key)
        elif choice == '4':
            row_key = input("Enter Row Key to query: ")
            query_by_id(conn, 'recruit', row_key)
        elif choice == '5':
            conditions = get_conditions()
            query_by_field(conn, 'recruit', conditions)
        elif choice == '6':
            column = input("Enter column to query (with 'info:'): ")
            pattern = input("Enter prefix pattern: ")
            fuzzy_query(conn, 'recruit', column, pattern)
        elif choice == '0':
            print("Exiting program.")
            break
        else:
            print("Invalid choice, please choose again.")
    
    conn.close()

if __name__ == '__main__':
    main()
```

##### 3.4.2.1 插入数据

```python
def insert_data(connection, table_name, row_key, data_dict):
    try:
        table = connection.table(table_name)
        with table.batch() as b:
            b.put(row_key.encode('utf-8'), {k.encode('utf-8'): str(v).encode('utf-8') for k, v in data_dict.items()})
        print("Data inserted successfully.")
    except Exception as e:
        print(f"Error inserting data: {e}")
```

##### 3.4.2.2 删除数据

```python
def delete_data(connection, table_name, row_key):
    try:
        table = connection.table(table_name)
        table.delete(row_key.encode('utf-8'))
        print("Data deleted successfully.")
    except Exception as e:
        print(f"Error deleting data: {e}")
```

##### 3.4.2.3 修改数据

```python
def update_data(connection, table_name, row_key, data_dict):
    try:
        table = connection.table(table_name)
        with table.batch() as b:
            for field, value in data_dict.items():
                b.put(row_key.encode('utf-8'), {field.encode('utf-8'): str(value).encode('utf-8')})
        print("Data updated successfully.")
    except Exception as e:
        print(f"Error updating data: {e}")
```

##### 3.4.2.4 查询数据

###### 3.4.2.4.1 按城市与公司类型查询

```python
def query_by_field(connection, table_name, conditions):
    try:
        table = connection.table(table_name)
        filters = [f"SingleColumnValueFilter('info', '{field}', =, 'binary:{value}')" for field, value in conditions.items()]
        filter_str = " AND ".join(filters)
        rows = table.scan(filter=filter_str)
        for key, data in rows:
            print(f"Row Key: {key.decode('utf-8')}")
            for col, val in data.items():
                print(f"  {col.decode('utf-8')}: {val.decode('utf-8')}")
    except Exception as e:
        print(f"Error querying by field: {e}")
```

###### 3.4.2.4.2 按城市与岗位关键字与公司类型查询




## 4.大数据统计数据分析

### 4.1 Mapreduce 数据分析
#### 4.1.1 统计每个学历要求出现的次数

- 分析目的：分析企业对求职者的学历要求分布，了解不同岗位的最低学历门槛，帮助求职者评估自身竞争力，同时为企业优化招聘策略提供参考。

- 代码：
```java
package bigdata;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class EducationWordCount {
    public static class EducationMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text education = new Text();
        private Set<String> allowedEducations = new HashSet<>();
        private boolean isHeader = true;

        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            allowedEducations.add("中专/中技");
            allowedEducations.add("初中及以下");
            allowedEducations.add("博士");
            allowedEducations.add("大专");
            allowedEducations.add("学历不限");
            allowedEducations.add("本科");
        }

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 使用更健壮的表头判断方式
            if (isHeader) {
                isHeader = false;
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length > 5) { // 确保有足够的字段
                String educationRequirement = fields[5]; // 学历要求是第六个字段
                if (allowedEducations.contains(educationRequirement)) {
                    education.set(educationRequirement);
                    context.write(education, one);
                }
            }
        }
    }

    public static class EducationReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: EducationWordCount <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Education Word Count");
        job.setJarByClass(EducationWordCount.class);

        // 设置Mapper和Reducer
        job.setMapperClass(EducationMapper.class);
        job.setReducerClass(EducationReducer.class);

        // 设置输出类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 设置输入和输出路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

- 运行情况：
![[image-20250625165440873.png]]
![[image-20250625165502372.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/EducationWordCount
hadoop jar EducationWordCount.jar /data/recruit /data/mr/EducationWordCount
hdfs dfs -ls /data/mr/EducationWordCount
hdfs dfs -cat /data/mr/EducationWordCount/part-r-00000
```

![[image-20250625165513627.png]]

- 结论：


#### 4.1.2 统计各岗位在各城市的平均薪资 top5

- 分析目的：对比不同城市相同岗位的薪资水平，揭示高薪岗位聚集地，帮助求职者选择高薪地区，同时为企业调整薪资竞争力提供数据支持。

- 代码：
```java
package bigdata;

import java.io.IOException;
import java.util.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class JobCitySalaryTop {

    // 第一阶段：计算各岗位在各城市的平均薪资
    public static class SalaryMapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text jobCity = new Text();
        private Text salary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 跳过表头行
            if (key.get() == 0) {
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length >= 4) { // 确保有足够的字段
                try {
                    String city = fields[1];  // 城市在第二列
                    String job = fields[2];   // 岗位在第三列
                    double salaryValue = Double.parseDouble(fields[3]);  // 薪资在第四列
                    
                    jobCity.set(job + "\t" + city);
                    salary.set(String.valueOf(salaryValue));
                    context.write(jobCity, salary);
                } catch (NumberFormatException e) {
                    System.err.println("Invalid salary value: " + value.toString());
                }
            }
        }
    }

    public static class SalaryReducer extends Reducer<Text, Text, Text, Text> {
        private Text job = new Text();
        private Text cityAvgSalary = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            String[] parts = key.toString().split("\t");
            String jobName = parts[0];
            String cityName = parts[1];
            
            double sum = 0;
            int count = 0;
            for (Text val : values) {
                sum += Double.parseDouble(val.toString());
                count++;
            }
            
            double avgSalary = sum / count;
            
            job.set(jobName);
            cityAvgSalary.set(cityName + "\t" + String.format("%.2f", avgSalary));
            context.write(job, cityAvgSalary);
        }
    }

    // 第二阶段：对每个岗位的城市按平均薪资排序并取Top5
    public static class Top5Mapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text job = new Text();
        private Text citySalary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split("\t");
            if (parts.length >= 3) {
                job.set(parts[0]);  // 岗位
                citySalary.set(parts[1] + "\t" + parts[2]);  // 城市和平均薪资
                context.write(job, citySalary);
            }
        }
    }

    public static class Top5Reducer extends Reducer<Text, Text, NullWritable, Text> {
        private Text output = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // 使用TreeMap按平均薪资降序排序
            TreeMap<Double, List<String>> salaryToCities = new TreeMap<>(Collections.reverseOrder());
            
            for (Text val : values) {
                String[] parts = val.toString().split("\t");
                String city = parts[0];
                double salary = Double.parseDouble(parts[1]);
                
                salaryToCities.computeIfAbsent(salary, k -> new ArrayList<>()).add(city);
            }
            
            // 输出Top5
            int topCount = 0;
            for (Map.Entry<Double, List<String>> entry : salaryToCities.entrySet()) {
                double salary = entry.getKey();
                List<String> cities = entry.getValue();
                
                for (String city : cities) {
                    if (topCount >= 5) {
                        break;
                    }
                    
                    output.set(key.toString() + "\t" + city + "\t" + String.format("%.2f", salary));
                    context.write(NullWritable.get(), output);
                    topCount++;
                }
                
                if (topCount >= 5) {
                    break;
                }
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        
        if (otherArgs.length != 3) {
            System.err.println("Usage: JobCitySalaryTop <input path> <temp path> <output path>");
            System.exit(2);
        }

        // 第一阶段作业：计算平均薪资
        Job job1 = Job.getInstance(conf, "JobCitySalaryAvg");
        job1.setJarByClass(JobCitySalaryTop.class);
        
        job1.setMapperClass(SalaryMapper.class);
        job1.setReducerClass(SalaryReducer.class);
        
        job1.setMapOutputKeyClass(Text.class);
        job1.setMapOutputValueClass(Text.class);
        
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job1, new Path(otherArgs[1]));
        
        boolean success = job1.waitForCompletion(true);
        if (!success) {
            System.exit(1);
        }

        // 第二阶段作业：取Top5
        Job job2 = Job.getInstance(conf, "JobCitySalaryTop5");
        job2.setJarByClass(JobCitySalaryTop.class);
        
        job2.setMapperClass(Top5Mapper.class);
        job2.setReducerClass(Top5Reducer.class);
        
        job2.setMapOutputKeyClass(Text.class);
        job2.setMapOutputValueClass(Text.class);
        
        job2.setOutputKeyClass(NullWritable.class);
        job2.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job2, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job2, new Path(otherArgs[2]));
        
        System.exit(job2.waitForCompletion(true) ? 0 : 1);
    }
}
```

- 运行情况：
![[image-20250625230541767.png]]
![[image-20250625230555621.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/JobCitySalaryTop
hadoop jar JobCitySalaryTop.jar /data/recruit /data/temp/JobCitySalaryTop /data/mr/JobCitySalaryTop
hdfs dfs -ls /data/mr/JobCitySalaryTop
hdfs dfs -cat /data/mr/JobCitySalaryTop/part-r-00000
```
![[image-20250625230451123.png]]

- 结论：

#### 4.1.3 筛选各城市薪资高于 10k 的职位数

- 分析目的：衡量各城市高薪岗位的供给情况，反映城市经济活力和行业薪资水平，帮助求职者判断高薪机会分布，同时为企业制定薪酬策略提供参考。

- 代码：
```java
package xyz.hiubo.bigdata;  
  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class CityHighSalaryJobsCount {  
  
    // Mapper：筛选薪资>10k的记录，并按城市分组  
    public static class HighSalaryMapper extends Mapper<LongWritable, Text, Text, LongWritable> {  
        private Text city = new Text();  
        private final LongWritable one = new LongWritable(1);  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 4) { // 确保有薪资字段  
                try {  
                    String cityName = fields[1];  // 城市在第二列  
                    double salary = Double.parseDouble(fields[3]);  // 薪资在第四列  
                    // 筛选薪资>10k的记录  
                    if (salary > 10.0) {  
                        city.set(cityName);  
                        context.write(city, one);  
                    }  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Reducer：统计每个城市的符合条件职位数  
    public static class HighSalaryReducer extends Reducer<Text, LongWritable, Text, LongWritable> {  
        private LongWritable result = new LongWritable();  
  
        @Override  
        protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {  
            long count = 0;  
            for (LongWritable val : values) {  
                count += val.get();  
            }  
            result.set(count);  
            context.write(key, result);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: CityHighSalaryJobsCount <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "City High Salary Jobs Count");  
        job.setJarByClass(CityHighSalaryJobsCount.class);  
          
        job.setMapperClass(HighSalaryMapper.class);  
        job.setReducerClass(HighSalaryReducer.class);  
          
        job.setMapOutputKeyClass(Text.class);  
        job.setMapOutputValueClass(LongWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(LongWritable.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

- 运行情况：
![[image-20250625231431977.png]]
![[image-20250625231442455.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/CityHighSalaryJobsCount
hadoop jar CityHighSalaryJobsCount.jar /data/recruit /data/mr/CityHighSalaryJobsCount
hdfs dfs -ls /data/mr/CityHighSalaryJobsCount
hdfs dfs -cat /data/mr/CityHighSalaryJobsCount/part-r-00000
```
![[image-20250625231454624.png]]

- 结论：

#### 4.1.4 统计不同岗位不同学历不同经验的平均薪资

分析的目的：量化学历、经验对薪资的影响，帮助求职者规划职业发展路径（如是否提升学历或积累经验），同时为企业优化薪酬体系提供数据支撑。

- 代码：
```java
package xyz.hiubo.bigdata;  
  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.*;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class JobEducationExperienceSalaryAvg {  
  
    // 自定义组合键：岗位、学历和经验  
    public static class JobEducationExperienceKey implements WritableComparable<JobEducationExperienceKey> {  
        private Text job;  
        private Text education;  
        private Text experience;  
  
        public JobEducationExperienceKey() {  
            this.job = new Text();  
            this.education = new Text();  
            this.experience = new Text();  
        }  
  
        public JobEducationExperienceKey(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public void set(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public Text getJob() {  
            return job;  
        }  
  
        public Text getEducation() {  
            return education;  
        }  
  
        public Text getExperience() {  
            return experience;  
        }  
  
        @Override  
        public void write(DataOutput out) throws IOException {  
            job.write(out);  
            education.write(out);  
            experience.write(out);  
        }  
  
        @Override  
        public void readFields(DataInput in) throws IOException {  
            job.readFields(in);  
            education.readFields(in);  
            experience.readFields(in);  
        }  
  
        @Override  
        public int compareTo(JobEducationExperienceKey other) {  
            int jobCompare = job.compareTo(other.job);  
            if (jobCompare != 0) {  
                return jobCompare;  
            }  
              
            int educationCompare = education.compareTo(other.education);  
            if (educationCompare != 0) {  
                return educationCompare;  
            }  
              
            return experience.compareTo(other.experience);  
        }  
  
        @Override  
        public int hashCode() {  
            return job.hashCode() * 163 * 163 + education.hashCode() * 163 + experience.hashCode();  
        }  
  
        @Override  
        public boolean equals(Object o) {  
            if (this == o) return true;  
            if (o == null || getClass() != o.getClass()) return false;  
            JobEducationExperienceKey that = (JobEducationExperienceKey) o;  
            return job.equals(that.job) &&   
                   education.equals(that.education) &&   
                   experience.equals(that.experience);  
        }  
  
        @Override  
        public String toString() {  
            return job + "\t" + education + "\t" + experience;  
        }  
    }  
  
    // Mapper：提取岗位、学历、经验和薪资信息  
    public static class SalaryMapper extends Mapper<LongWritable, Text, JobEducationExperienceKey, DoubleWritable> {  
        private JobEducationExperienceKey key = new JobEducationExperienceKey();  
        private DoubleWritable salary = new DoubleWritable();  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 5) { // 确保有足够的字段  
                try {  
                    String job = fields[2];          // 岗位在第三列  
                    String education = fields[5];    // 学历在第六列  
                    String experience = fields[4];   // 经验在第五列  
                    double salaryValue = Double.parseDouble(fields[3]); // 薪资在第四列  
                    this.key.set(new Text(job), new Text(education), new Text(experience));  
                    salary.set(salaryValue);  
                    context.write(this.key, salary);  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Combiner：在Map端进行局部聚合，减少数据传输  
    public static class SalaryCombiner extends Reducer<JobEducationExperienceKey, DoubleWritable, JobEducationExperienceKey, DoubleWritable> {  
        private DoubleWritable result = new DoubleWritable();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            result.set(sum / count);  
            context.write(key, result);  
        }  
    }  
  
    // Reducer：计算平均薪资  
    public static class SalaryReducer extends Reducer<JobEducationExperienceKey, DoubleWritable, Text, Text> {  
        private Text outputKey = new Text();  
        private Text outputValue = new Text();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            double avgSalary = sum / count;  
              
            outputKey.set(key.getJob() + "\t" + key.getEducation() + "\t" + key.getExperience());  
            outputValue.set(String.format("%.2f", avgSalary));  
            context.write(outputKey, outputValue);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: JobEducationExperienceSalaryAvg <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "Job Education Experience Salary Average");  
        job.setJarByClass(JobEducationExperienceSalaryAvg.class);  
          
        job.setMapperClass(SalaryMapper.class);  
        job.setCombinerClass(SalaryCombiner.class);  
        job.setReducerClass(SalaryReducer.class);  
          
        job.setMapOutputKeyClass(JobEducationExperienceKey.class);  
        job.setMapOutputValueClass(DoubleWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(Text.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

- 运行情况：
![[image-20250625232401879.png]]
![[image-20250625232420680.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/JobEducationExperienceSalaryAvg
hadoop jar JobEducationExperienceSalaryAvg.jar /data/recruit /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -ls /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -cat /data/mr/JobEducationExperienceSalaryAvg/part-r-00000
```
![[image-20250625232458992.png]]
- 结论：

### 4.2 Hive 数据分析

- 启动 Hive
```shell
hive
```
![[image-20250625234158949.png]]

- 创建 Hive 外部表并验证
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS recruit (
  id INT,
  city STRING,
  position STRING,
  salary DOUBLE,
  experience DOUBLE,
  education STRING,
  company_name STRING,
  company_type STRING,
  company_size INT,
  create_time STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/data/recruit/'
TBLPROPERTIES ("skip.header.line.count"="1");
-- 验证
SELECT * FROM recruit LIMIT 5;
```
![[image-20250625234413525.png]]

#### 4.2.1 统计各城市各岗位的平均薪资 

- 分析目的：构建城市-岗位薪资矩阵，帮助求职者对比不同地区的薪资差异，同时为企业制定区域差异化薪酬政策提供数据支持。

- 代码：
```sql
SELECT 
  position AS position_name,
  city AS city_name,
  percentile_approx(salary, 0.5) AS median_salary
FROM recruit
WHERE salary IS NOT NULL
GROUP BY position, city
ORDER BY position_name, median_salary DESC;
```

- 运行情况：
![[image-20250626161526222.png]]

- 查看结果：
![[image-20250626161606697.png]]

- 结论：

#### 4.2.2 计算不同学历的平均薪资

- 分析目的：量化学历对薪资的影响程度，帮助求职者评估学历提升的投资回报率，同时为企业优化薪酬结构（如学历补贴政策）提供参考。

- 代码：
```sql
SELECT education,AVG(salary) AS avg_salary
	FROM recruit
	GROUP BY education
	ORDER BY avg salary DESC;
```

- 运行情况：
![[image-20250626161651352.png]]

- 查看结果：
![[image-20250626161707064.png]]

- 结论：

#### 4.2.3 按月份统计岗位数量趋势

- 分析目的：分析招聘市场的季节性波动，识别招聘旺季和淡季，帮助求职者把握最佳求职时机，同时为企业优化招聘节奏提供依据。

- 代码：
```sql
SELECT 
    SUBSTR(create_time, 1, 7) AS month,
    COUNT(*) AS post_count
FROM recruit
GROUP BY SUBSTR(create_time, 1, 7)
ORDER BY month;
```

- 运行情况：
![[image-20250626161042697.png]]

- 查看结果：
![[image-20250626161059976.png]]

- 结论：

#### 4.2.4 统计公司类型的分布

- 分析目的：分析不同类型企业（如国企、外企、创业公司）的招聘偏好，帮助求职者选择适合的企业类型，同时为企业研究市场竞争格局提供数据支持。

- 代码：
```sql
SELECT 
    company_type,
    COUNT(*) AS total_positions,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage
FROM recruit
GROUP BY company_type
ORDER BY total_positions DESC;
```

- 运行情况：
![[image-20250626161222800.png]]

- 查看结果：
![[image-20250626161242867.png]]

- 结论：

### 4.3 Spark SQL 数据分析
#### 4.3.1查询各个行业最高薪资

- 分析目的：锁定顶尖高薪职位，激发求职者挑战高薪岗位的意愿，引导其向高薪职位所需技能和经验方向发展；同时促使企 业审视自身核心岗位的薪资竞争力，防止人才流失。

- 代码：
```sql
val maxSalaryPerJob = sqlContext.sql("SELECT job, MAX(salary) AS max_salary FROM recruit GROUP BY job ORDER BY max_salary DESC")
maxSalaryPerJob.show()
maxSalaryPerJob.write.format("com.databricks.spark.csv").option("header", "true").save("hdfs:///recruit/max_salary_per_job")

```

- 查看结果：
```
hdfs dfs -cat /recruit/max_salary_per_job/part*
```

![[Pasted image 20251106201844.png|475]]

#### 4.3.2 **统计各岗位各城市薪资中位数**

- 分析目的：相较于平均值，中位数更能反映各岗位各城市薪资的真实集中水平，避免极端值干扰，为求职者提供更准确的薪资参考，帮助企业合理定价岗位薪酬

- 代码：
```sql
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
object CollectListUDAF extends UserDefinedAggregateFunction {
  def inputSchema: StructType = StructType(StructField("value", DoubleType) :: Nil)
  def bufferSchema: StructType = StructType(StructField("list", ArrayType(DoubleType)) :: Nil)
  def dataType: DataType = ArrayType(DoubleType)
  def deterministic: Boolean = true
  
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = Seq.empty[Double]
  }
  
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      val list = buffer.getAs[Seq[Double]](0)
      buffer(0) = list :+ input.getDouble(0)
    }
  }
  
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    val list1 = buffer1.getAs[Seq[Double]](0)
    val list2 = buffer2.getAs[Seq[Double]](0)
    buffer1(0) = list1 ++ list2
  }
  
  def evaluate(buffer: Row): Any = {
    buffer.getSeq[Double](0)
  }
}
val medianUDF: UserDefinedFunction = udf { salaries: Seq[Double] =>
  val sorted = salaries.filter(_.isInstanceOf[Double]).map(_.asInstanceOf[Double]).sorted
  if (sorted.isEmpty) None
  else {
    val mid = sorted.length / 2
    if (sorted.length % 2 == 0) {
      Some((sorted(mid - 1) + sorted(mid)) / 2.0)
    } else {
      Some(sorted(mid))
    }
  }
}
sqlContext.udf.register("collect_list", CollectListUDAF)
val medianSalaryPerJobPerCity = sqlContext.sql("SELECT job, city, salary FROM recruit").groupBy("job", "city").agg(collect_list("salary").alias("salaries")).withColumn("median_salary", medianUDF(col("salaries"))).drop("salaries").orderBy("job", "median_salary")
medianSalaryPerJobPerCity.show(medianSalaryPerJobPerCity.count().toInt)
medianSalaryPerJobPerCity.write.format("com.databricks.spark.csv").option("header", "true").save("hdfs:///recruit/median_salary_per_job_per_city")
```

- 运行情况：
![[Pasted image 20251106202316.png|187]]

查看结果：

```shell
hdfs dfs -cat /recruit/median_salary_per_job_per_city/part*
```

![[Pasted image 20251106202559.png|175]]

#### 4.3.3 **查询互联网行业薪资最高的** **top20**  **公司**

- 分析目的：为互联网行业求职者提供具有吸引力的优质雇主参考，助力其锁定目标企业；同时激励互联网企业提升薪资福利 水平和雇主品牌建设，以吸引更多优秀人才。

- 代码：
```sql
val topSalariesInInternetIndustry = sqlContext.sql(""" SELECT job, company, MAX(salary) AS max_salary FROM recruit
WHERE industry = '互联网' GROUP BY job, company
ORDER BY max_salary DESC LIMIT 20
""")
topSalariesInInternetIndustry.show()
topSalariesInInternetIndustry.write.format("csv").option("header", "true").save("hdfs:///recruit/top_salaries_in_internet_industry")
```

- 运行情况：
![[Pasted image 20251106202624.png|250]]

- 查看结果：
```shell
hdfs dfs -cat /recruit/top_salaries_in_internet_industry/part*
```

![[Pasted image 20251106202641.png|275]]
#### 4.3.4 **按月份统计岗位数量趋势**

- 分析目的：把握招聘市场动态变化规律，揭示不同月份岗位供需波动情况，为求职者规划求职时间提供依据，使其能在招聘 旺季增加就业机会；帮助企业提前规划招聘周期和人力配置，合理安排招聘预算和进度。


- 代码：
```sql
val jobTrendPerMonth = sqlContext.sql("""
SELECT DATE_FORMAT(post_time, 'yyyy-MM') AS month, COUNT(*) AS job_count FROM recruit
GROUP BY DATE_FORMAT(post_time, 'yyyy-MM')
ORDER BY month
""")
jobTrendPerMonth.show()
jobTrendPerMonth.write.format("csv").option("header", "true").save("hdfs:///recruit/job_trend_per_month")
```

- 运行情况：
![[Pasted image 20251106202701.png|122]]

- 查看结果：
```shell
hdfs dfs -cat /recruit/job_trend_per_month/part*
```

![[Pasted image 20251106202716.png|225]]


## 5.实时数据分析

### 5.1 Flink 流数据实时分析
#### 5.1.1 实时统计城市岗位数量

- 分析目的：动态监测各城市的招聘需求变化，帮助求职者快速捕捉最新机会，同时为企业实时调整招聘策略（如区域扩张或收缩）提供决策依据。

- 代码：
```Java
package xyz.hiubo.bigdata;  
  
import org.apache.flink.api.common.eventtime.WatermarkStrategy;  
import org.apache.flink.api.common.functions.FlatMapFunction;  
import org.apache.flink.api.java.tuple.Tuple3;  
import org.apache.flink.configuration.Configuration;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.connector.file.src.FileSource;  
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;  
import org.apache.flink.core.fs.Path;  
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;  
import org.apache.flink.streaming.api.windowing.time.Time;  
import org.apache.flink.util.Collector;  
  
import java.time.Duration;  
  
public class CityJobCount {  
  
    public static void main(String[] args) throws Exception {  
  
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
        env.setParallelism(1);  
  
        // 啟用 checkpoint 以支持持續監聽新文件  
        env.enableCheckpointing(10000);  
  
        // 使用 TextLineInputFormat 按行讀取文本文件  
        FileSource<String> fileSource = FileSource.forRecordStreamFormat(  
                        new TextLineInputFormat(),  
                        new Path("/home/hiubo/data/recruit/stream1")  
                )  
                .monitorContinuously(Duration.ofSeconds(5))  
                .build();  
  
        DataStream<String> lines = env.fromSource(fileSource, WatermarkStrategy.noWatermarks(), "File Source");  
  
        // 解析每行為 (city, job, count)        DataStream<Tuple3<String, String, Integer>> parsed = lines.flatMap(  
                new FlatMapFunction<String, Tuple3<String, String, Integer>>() {  
                    @Override  
                    public void flatMap(String line, Collector<Tuple3<String, String, Integer>> out) {  
                        String[] parts = line.split(",");  
                        if (parts.length == 2) {  
                            String city = parts[0].trim();  
                            String job = parts[1].trim();  
                            out.collect(Tuple3.of(city, job, 1));  
                        }  
                    }  
                }  
        );  
  
        // 按照 (city, job) 分組並統計數量  
        parsed.keyBy(value -> value.f0 + ":" + value.f1)  
                .sum(2)  
                .print();  
  
        env.execute("City Job Count Streaming");  
    }  
}
```

- 运行情况：
![[Pasted image 20250628174143.png]]

![[Pasted image 20250628174208.png]]

![[Pasted image 20250628174227.png]]

- 结论：


#### 5.1.2 实时统计行业分布

- 分析目的：对最新招聘数据进行实时处理和监控，及时掌握各行业人才需求变化趋势，为行业分析和市场预测提供最及时的 数据支持；确保招聘平台能够第一时间根据行业动态调整推荐策略，满足不同行业的人才招聘需求。

- 消费者：
```java
package xyz.hiubo.bigdata;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.util.Collector;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import java.nio.charset.StandardCharsets;
import java.util.Properties;

public class FlinkKafkaJobStats {
    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "flink-consumer-group");
        properties.setProperty("auto.offset.reset", "earliest");

        // 自定义 Kafka Deserialization Schema
        KafkaDeserializationSchema<String> deserializer = new KafkaDeserializationSchema<String>() {
            @Override
            public boolean isEndOfStream(String nextElement) {
                return false;
            }

            @Override
            public String deserialize(ConsumerRecord<byte[], byte[]> record) {
                if (record.value() == null) {
                    return null;
                }
                return new String(record.value(), StandardCharsets.UTF_8);
            }

            @Override
            public TypeInformation<String> getProducedType() {
                return TypeInformation.of(String.class);
            }
        };

        DataStream<String> kafkaStream = env.addSource(
                new FlinkKafkaConsumer<>("industry-job", deserializer, properties)
        );

        DataStream<Tuple2<String, Integer>> parsedStream = kafkaStream
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
                        String industry = value.trim();
                        if (!industry.isEmpty()) {
                            out.collect(Tuple2.of(industry, 1));
                        }
                    }
                });

        parsedStream.keyBy(value -> value.f0)
                .timeWindow(Time.seconds(5))
                .sum(1)
                .print();

        env.execute("Flink Kafka Industry Job Count");
    }
}
```

- 生产者：
```java
package xyz.hiubo.bigdata;

import org.apache.kafka.clients.producer.*;
import com.opencsv.CSVReader;
import java.io.FileReader;
import java.util.Properties;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class KafkaCsvProducer {

    private static final String TOPIC = "industry-job";
    private static final String CSV_FILE_PATH = "/home/hiubo/data/recruit/industry_job.csv";

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        Producer<String, String> producer = new KafkaProducer<>(props);

        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.scheduleAtFixedRate(() -> {
            try (CSVReader reader = new CSVReader(new FileReader(CSV_FILE_PATH))) {
                int count = 0;
                String[] nextLine;
                while ((nextLine = reader.readNext()) != null && count++ < 1000) {
                    String industry = nextLine[0]; // 第一列是行业
                    producer.send(new ProducerRecord<>(TOPIC, industry));
                }
                System.out.println("Sent 1000 records to Kafka.");
            } catch (Exception e) {
                e.printStackTrace();
            }
        }, 0, 5, TimeUnit.SECONDS);
    }
}
```

- 运行情况：
![[Pasted image 20250628220809.png|204]]

![[Pasted image 20250628215935.png|135]]

![[Pasted image 20250628215949.png|160]]

![[Pasted image 20250628215958.png|160]]

- 结论：

### 5.2 Spark 流数据实时分析

#### 5.2.1 将预处理后的数据上传到指定文件夹中

```shell
cd /usr/local/bigdata/da
```

![[Pasted image 20251106203900.png|400]]

#### 5.2.2 启动kafka

```shell
cd /usr/local/kafka
bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
bin/kafka-server-start.sh -daemon config/server.properties
```
![[Pasted image 20251106203929.png|500]]
![[Pasted image 20251106203933.png|500]]
#### 5.2.3 创建topic并将数据上传到kafka中

```shell
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic recruit_topic < /usr/local/bigdata/da/recruit.csv
bin/kafka-console-producer.sh --create --zookeeper localhost:2181 --replication-factor 1--partitions 1--topic salary_max_topic
```
![[Pasted image 20251106203943.png|475]]
#### 5.2.4 启动spark并编写用于分析的spark代码

```shell
cd /usr/local/spark
 ./spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.0.0,org.apache.kafka:kafka-clients:0.10.0.0:paste
```

```scala
import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord, ProducerConfig}

// 初始化StreamingContext
val ssc = new StreamingContext(sc, Seconds(5))

// 设置检查点目录
ssc.checkpoint("file:///tmp/spark-checkpoint")

// 创建检查点目录（如果不存在）
import java.io.File
new File("/tmp/spark-checkpoint").mkdirs()

// Kafka 消费者配置
val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "salary_stats_group",
  "auto.offset.reset" -> "latest",
  "enable.auto.commit" -> (false: java.lang.Boolean)
)

// 定义消费主题
val topics = Array("recruit_topic")

// 创建 Kafka 输入流
val stream = KafkaUtils.createDirectStream[String, String](
  ssc,
  LocationStrategies.PreferConsistent,
  ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
)

// 定义状态更新函数用于比较最大值
def updateMaxSalary(newValues: Seq[Double], state: Option[Double]): Option[Double] = {
  val currentMax = newValues.reduceOption(_ max _)
  val previousMax = state
  (currentMax, previousMax) match {
    case (Some(current), Some(previous)) => Some(current max previous)
    case (Some(current), None) => Some(current)
    case (None, Some(previous)) => Some(previous)
    case (None, None) => None
  }
}

// 处理逻辑：解析数据、更新状态
val processedStream = stream
  .map(record => {
    val parts = record.value.split(",") // 假设数据格式为 "id,城市,岗位,薪资,年限,学历要求,公司名称,公司类型,公司规模,创建时间"
    if (parts.length >= 4 && parts(3).nonEmpty) {
        try {
            (parts(1), parts(3).toDouble) // (城市, 薪资)
        } catch {
            case e: NumberFormatException =>
                println(s"Invalid salary value: ${parts(3)}")
                (parts(1), 0.0) // 或者选择跳过该记录
        }
    } else {
        (parts(1), 0.0) // 或者选择跳过该记录
    }
  })
  .updateStateByKey(updateMaxSalary)

// 定义 Kafka 生产者配置
val producerConfig = new java.util.Properties()
producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092")
producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer")
producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer")

// 将结果发送回 Kafka
processedStream.foreachRDD { rdd =>
  rdd.foreachPartition { partition =>
    val producer = new KafkaProducer[String, String](producerConfig)
    partition.foreach { case (city, maxSalary) =>
      val message = s"$city,$maxSalary"
      val record = new ProducerRecord[String, String]("salary_max_topic", city, message)
      producer.send(record)
    }
    producer.close()
  }
}

// 启动流处理
ssc.start()
ssc.awaitTermination()
```
![[Pasted image 20251106204002.png]]
### 5.3 流数据实时分析与前端数据实时可视化

#### 5.3.1 编写用于可视化的python代码和index.html并运行

```python
import eventlet
eventlet.monkey_patch()

import threading
import time
import logging
import json

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

from kafka import KafkaConsumer
from flask import Flask, render_template, request
from flask_socketio import SocketIO, emit

# 创建Flask应用
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'

# 强制Flask使用UTF-8编码
app.config['JSON_AS_ASCII'] = False
app.config['JSONIFY_MIMETYPE'] = 'application/json; charset=utf-8'

socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    async_mode='eventlet',
    logger=True,
    engineio_logger=True
)

# Kafka配置
KAFKA_BOOTSTRAP_SERVERS = 'localhost:9092'
KAFKA_TOPIC = 'salary_max_topic'
KAFKA_GROUP_ID = 'flask-socketio-consumer'

# 全局标志和线程
kafka_consumer_thread = None
kafka_consumer_running = threading.Event()

def kafka_consumer_worker():
    logger.info(f"Kafka消费者线程启动，连接到 {KAFKA_BOOTSTRAP_SERVERS}，监听主题 {KAFKA_TOPIC}")
    
    try:
        consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=KAFKA_GROUP_ID,
            auto_offset_reset='latest',
            enable_auto_commit=True,
            value_deserializer=lambda x: x.decode('utf-8')
        )
        
        for message in consumer:
            if not kafka_consumer_running.is_set():
                break
                
            try:
                data = message.value
                city, salary_str = data.split(',')
                salary = int(float(salary_str.strip()))
                
                logger.info(f"从Kafka接收到: 城市={city.strip()}, 薪资={salary}")
                
                # 手动序列化JSON，确保中文字符不转义
                data_dict = {'city': city.strip(), 'salary': salary}
                data_json = json.dumps(
                    data_dict,
                    ensure_ascii=False,  # 禁用ASCII转义，关键参数
                    separators=(',', ':')  # 优化JSON格式
                )
                
                # 打印序列化后的JSON用于调试
                logger.debug(f"发送的JSON数据: {data_json}")
                
                # 发送JSON字符串
                socketio.emit('new_salary_data', data_json)
                
            except ValueError as e:
                logger.warning(f"数据格式错误: {data}, 错误: {e}")
            except Exception as e:
                logger.error(f"处理Kafka消息时发生错误: {e}")
    
    except Exception as e:
        logger.error(f"Kafka消费者初始化或运行时发生严重错误: {e}")
    finally:
        if 'consumer' in locals() and consumer.bootstrap_connected():
            consumer.close()
        kafka_consumer_running.clear()

@app.route('/')
def index():
    return render_template('index.html')

@socketio.on('connect')
def handle_connect():
    sid = request.sid
    logger.info(f'客户端已连接: {sid}')
    
    if not kafka_consumer_running.is_set():
        logger.info("首次客户端连接，启动Kafka消费者线程...")
        global kafka_consumer_thread
        kafka_consumer_thread = threading.Thread(target=kafka_consumer_worker)
        kafka_consumer_thread.daemon = True
        kafka_consumer_running.set()
        kafka_consumer_thread.start()

@socketio.on('disconnect')
def handle_disconnect():
    sid = request.sid
    logger.info(f'客户端已断开: {sid}')

@app.route('/stop_kafka_consumer')
def stop_consumer_route():
    if kafka_consumer_running.is_set():
        logger.info("收到停止Kafka消费者线程的请求...")
        kafka_consumer_running.clear()
        return "Kafka消费者线程正在停止..."
    return "Kafka消费者线程未运行"

@app.route('/test')
def test_route():
    return "服务器正常运行", 200

if __name__ == '__main__':
    logger.info("Flask应用启动中...")
    logger.info(f"请访问 http://127.0.0.1:5000")
    
    try:
        socketio.run(app, host='0.0.0.0', port=5000, debug=False)
    finally:
        logger.info("Flask应用正在关闭，发送停止信号给Kafka消费者线程...")
        kafka_consumer_running.clear()
        if kafka_consumer_thread and kafka_consumer_thread.is_alive():
            kafka_consumer_thread.join(timeout=5)
        logger.info("Flask应用已关闭")
```

```html
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>实时城市薪资数据</title>
    <!-- Socket.IO 客户端库 -->
    <script src="https://cdn.socket.io/4.0.0/socket.io.min.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        h1 {
            text-align: center;
            color: #0056b3;
        }
        .chart-container {
            min-width: 310px;
            max-width: 800px;
            height: 400px;
            margin: 0 auto;
            background-color: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            position: relative;
            padding: 20px;
        }
        #status {
            text-align: center;
            margin-top: 10px;
            font-size: 0.9em;
            color: #666;
        }
        .status-connected { color: green; }
        .status-disconnected { color: red; }
        #loading {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 1.2em;
            color: #666;
        }
        #chartCanvas {
            width: 100%;
            height: 100%;
        }
        .bar {
            fill: #4e79a7;
            transition: width 0.5s ease;
        }
        .bar-label {
            font-size: 12px;
            text-anchor: end;
            fill: #333;
        }
        .value-label {
            font-size: 12px;
            text-anchor: start;
            fill: #333;
        }
        .axis-label {
            font-size: 14px;
            fill: #666;
        }
    </style>
</head>
<body>
    <h1>实时城市平均薪资看板</h1>
    <div id="status">连接状态: <span id="connection-status">未连接</span></div>
    <div class="chart-container">
        <div id="loading">正在加载数据...</div>
        <svg id="chartCanvas" viewBox="0 0 800 400" preserveAspectRatio="xMidYMid meet"></svg>
    </div>

    <script>
        // 确保在DOM加载完成后执行
        document.addEventListener('DOMContentLoaded', function() {
            // 获取DOM元素
            const connectionStatusSpan = document.getElementById('connection-status');
            const loadingElement = document.getElementById('loading');
            const chartCanvas = document.getElementById('chartCanvas');
            
            // 使用Map存储城市薪资数据
            const salaryData = new Map();
            
            // 连接到Socket.IO服务器
            const socket = io();
            
            // 更新图表数据
            function updateChart() {
                // 清空SVG画布
                chartCanvas.innerHTML = '';
                
                // 如果没有数据，显示加载提示
                if (salaryData.size === 0) {
                    loadingElement.style.display = 'block';
                    return;
                }
                
                // 隐藏加载提示
                loadingElement.style.display = 'none';
                
                // 将数据转换为数组并按薪资降序排序
                const sortedData = Array.from(salaryData.entries())
                    .sort((a, b) => b[1] - a[1]);
                
                // 计算最大值用于缩放
                const maxSalary = Math.max(...sortedData.map(item => item[1]));
                
                // 设置图表尺寸
                const width = 800;
                const height = 400;
                const margin = { top: 40, right: 40, bottom: 60, left: 120 };
                const chartWidth = width - margin.left - margin.right;
                const chartHeight = height - margin.top - margin.bottom;
                
                // 创建SVG组
                const svg = chartCanvas;
                
                // 添加标题
                const title = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                title.setAttribute('x', width / 2);
                title.setAttribute('y', 20);
                title.setAttribute('text-anchor', 'middle');
                title.setAttribute('font-size', '18px');
                title.setAttribute('font-weight', 'bold');
                title.textContent = '实时城市平均薪资分布';
                svg.appendChild(title);
                
                // 添加Y轴标签
                const yAxisLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                yAxisLabel.setAttribute('x', -(height / 2));
                yAxisLabel.setAttribute('y', 20);
                yAxisLabel.setAttribute('text-anchor', 'middle');
                yAxisLabel.setAttribute('transform', 'rotate(-90)');
                yAxisLabel.setAttribute('font-size', '14px');
                yAxisLabel.setAttribute('class', 'axis-label');
                yAxisLabel.textContent = '城市';
                svg.appendChild(yAxisLabel);
                
                // 添加X轴标签
                const xAxisLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                xAxisLabel.setAttribute('x', width / 2);
                xAxisLabel.setAttribute('y', height - 10);
                xAxisLabel.setAttribute('text-anchor', 'middle');
                xAxisLabel.setAttribute('font-size', '14px');
                xAxisLabel.setAttribute('class', 'axis-label');
                xAxisLabel.textContent = '薪资 (元)';
                svg.appendChild(xAxisLabel);
                
                // 添加网格线
                const gridGroup = document.createElementNS('http://www.w3.org/2000/svg', 'g');
                gridGroup.setAttribute('class', 'grid');
                
                // 绘制柱状图
                const barHeight = 25;
                const spacing = 10;
                
                sortedData.forEach(([city, salary], index) => {
                    const y = margin.top + index * (barHeight + spacing);
                    
                    // 计算柱子宽度
                    const barWidth = (salary / maxSalary) * chartWidth;
                    
                    // 创建柱子
                    const bar = document.createElementNS('http://www.w3.org/2000/svg', 'rect');
                    bar.setAttribute('x', margin.left);
                    bar.setAttribute('y', y);
                    bar.setAttribute('width', barWidth);
                    bar.setAttribute('height', barHeight);
                    bar.setAttribute('class', 'bar');
                    bar.setAttribute('fill', `hsl(${(index * 40) % 360}, 70%, 60%)`);
                    svg.appendChild(bar);
                    
                    // 添加城市标签
                    const cityLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                    cityLabel.setAttribute('x', margin.left - 10);
                    cityLabel.setAttribute('y', y + barHeight / 2 + 5);
                    cityLabel.setAttribute('class', 'bar-label');
                    cityLabel.textContent = city;
                    svg.appendChild(cityLabel);
                    
                    // 添加薪资数值标签
                    const valueLabel = document.createElementNS('http://www.w3.org/2000/svg', 'text');
                    valueLabel.setAttribute('x', margin.left + barWidth + 5);
                    valueLabel.setAttribute('y', y + barHeight / 2 + 5);
                    valueLabel.setAttribute('class', 'value-label');
                    valueLabel.textContent = `${salary.toLocaleString()} 元`;
                    svg.appendChild(valueLabel);
                });
            }
            
            // 初始化图表
            updateChart();
            
            // Socket.IO事件监听
            socket.on('connect', function() {
                console.log('已连接到Socket.IO服务器');
                connectionStatusSpan.textContent = '已连接';
                connectionStatusSpan.className = 'status-connected';
            });
            
            socket.on('disconnect', function() {
                console.log('已从Socket.IO服务器断开');
                connectionStatusSpan.textContent = '已断开';
                connectionStatusSpan.className = 'status-disconnected';
            });
            
            // 监听来自后端的新薪资数据事件
            socket.on('new_salary_data', function(data) {
                console.log('收到新数据:', data);
                
                // 解析数据
                let dataObj;
                try {
                    // 如果数据是字符串则解析，否则直接使用
                    dataObj = typeof data === 'string' ? JSON.parse(data) : data;
                } catch (e) {
                    console.error('数据解析失败:', e);
                    return;
                }
                
                // 确保数据包含city和salary字段
                if (!dataObj || typeof dataObj !== 'object' || 
                    !dataObj.hasOwnProperty('city') || 
                    !dataObj.hasOwnProperty('salary')) {
                    console.error('无效的数据格式:', dataObj);
                    return;
                }
                
                const city = dataObj.city;
                const salary = Number(dataObj.salary);
                
                if (isNaN(salary)) {
                    console.error('薪资值无效:', dataObj.salary);
                    return;
                }
                
                console.log('城市:', city, '薪资:', salary);
                
                // 更新数据
                salaryData.set(city, salary);
                
                // 更新图表
                updateChart();
            });
        });
    </script>
</body>
</html>
```


#### 5.3.2 启动生产者访问对应的url

```shell
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic recruit_topic < /usr/local/bigdata/da/recruit.csv
```
![[Pasted image 20251106204118.png|400]]
## 6.高级数据分析

### 6.1 分类

本节指在使用随机森林实现按照招聘信息的薪资，年限，城市对岗位上的学历要求进行分类预测

#### 6.1.1 加载数据并查看数据信息

```shell
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.RandomForestClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler}
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{StructType, StructField, IntegerType, StringType, DoubleType}
import org.apache.spark.{SparkConf, SparkContext}

val sqlContext = new SQLContext(sc)

val schema = StructType(Array(
  StructField("id", IntegerType, true),
  StructField("城市", StringType, true),
  StructField("岗位", StringType, true),
  StructField("薪资", DoubleType, true),
  StructField("年限", DoubleType, true), 
  StructField("学历要求", StringType, true),
  StructField("公司名称", StringType, true),
  StructField("公司类型", StringType, true),
  StructField("公司规模", IntegerType, true),
  StructField("创建时间", StringType, true)
))

val recruitDF = sqlContext.read.format("com.databricks.spark.csv").schema(schema).option("header", "true").load("hdfs:///recruit/recruit.csv")

println("数据基本信息：")
recruitDF.printSchema()
println("数据集前2行：")
recruitDF.show(2, false)
```

运行结果如下:
![[Pasted image 20251106203311.png|250]]
![[Pasted image 20251106203324.png|350]]

#### 6.1.2 统计数据缺失值处理

这部分代码的作用是统计数据中各个字段的缺失值数量。通过对所有列进行遍历，统计出为空或者为 NULL 的记录数。从输出结果来看，数据中不存在缺失值，这就为后续直接开展模型训练提供了便利。

```shell
println("各字段缺失值统计：")
recruitDF.select(recruitDF.columns.map(c => count(when(col(c).isNull || col(c) === "", c)).alias(c)): _*).show()
```

输出结果如下：
![[Pasted image 20251106203337.png|525]]
#### 6.1.3 数据预处理与特征构造

该部分对数据进行了预处理和特征构造工作。对于 “薪资” 和 “年限” 字段，把值为 0 的数据分别替换成了 10.0 和 1.0，这样做是为了消除可能存在的异常值。同时，对 “公司规模”“城市”“岗位” 这三个字段进行了手动编码，将其转换为数值索引。例如，按照公司规模的不同范围将其映射为 0 到 3 的索引值，把不同城市映射为 0 到 4 的索引值，不同岗位映射为 0 到 5 的索引值。这种处理方式有助于模型更好地理解和利用这些分类特征。

```shell
val processedData = recruitDF.withColumn("薪资", when(col("薪资") === 0.0, lit(10.0)).otherwise(col("薪资"))).withColumn("年限", when(col("年限") === 0.0, lit(1.0)).otherwise(col("年限"))).withColumn("公司规模Index", when(col("公司规模") < 100, 0).when(col("公司规模") < 500, 1).when(col("公司规模") < 1000, 2).otherwise(3)).withColumn("城市Index", when(col("城市") === "广州", 0).when(col("城市") === "深圳", 1).when(col("城市") === "北京", 2).when(col("城市") === "上海", 3).otherwise(4)).withColumn("岗位Index", when(col("岗位") === "Java工程师", 0).when(col("岗位") === "产品经理", 1).when(col("岗位") === "数据分析师", 2).when(col("岗位") === "前端开发", 3).when(col("岗位") === "运营", 4).otherwise(5))
```

输出结果如下：
![[Pasted image 20251106203350.png|475]]
#### 6.1.4 构建特征和标签

此部分的任务是构建模型所需的特征向量和标签。把 “薪资” 和 “城市 Index” ，“年限” 这三个字段确定为特征列，利用 VectorAssembler 将它们组合成一个特征向量。对于 “学历要求” 字段，使用 StringIndexer 进行编码，将其转换为模型能够识别的标签列。最后，按照 7:3 的比例将数据划分为训练集和测试集，划分时使用了固定的随机种子，目的是保证结果具有可重复性

```scala
val featureCols = Array("城市Index", "薪资", "年限")
val eduIndexer = new StringIndexer().setInputCol("学历要求").setOutputCol("label").fit(processedData)
val assembler = new VectorAssembler().setInputCols(featureCols).setOutputCol("features")
val shuffledData = processedData.orderBy(rand(1234))
val Array(trainingData, testData) = shuffledData.randomSplit(Array(0.7, 0.3), seed = 1234)
```

#### 6.1.5 构建随机森林模型

这里创建了一个随机森林分类器，并且设置了相关参数。将标签列设置为之前处理好的 “label” 列，特征列设置为 “features” 列，同时设置了随机种子以确保结果可重现。随机森林是一种集成学习方法，它通过组合多个决策树来提高模型的准确性和稳定性。

```shell
val rf = new RandomForestClassifier().setLabelCol("label").setFeaturesCol("features").setSeed(1234)
```

#### 6.1.6 构建超参数调优

通过为随机森林模型设置了超参数网格，准备进行超参数调优。对两个重要的超参数进行了调整，分别是决策树的数量（提供了 10、20、30 这三个选项）和树的最大深度（提供了 5、10、15 这三个选项）。通过这种方式，可以找到最优的参数组合，从而提升模型的性能。

```shell
val paramGrid = new ParamGridBuilder().addGrid(rf.numTrees, Array(10, 20, 30)).addGrid(rf.maxDepth, Array(5, 10, 15)).build()
```


#### 6.1.7 构建交叉验证器

构建了一个交叉验证器，用于评估模型在不同参数组合下的性能。使用 F1 分数作为评估指标，这是因为它综合考虑了精确率和召回率，能够更全面地评价分类模型的性能。设置了 3 折交叉验证，这样可以更充分地利用数据，减少因数据划分不同而导致的评估偏差。
```shell
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("label")
  .setPredictionCol("prediction")
  .setMetricName("f1")

val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(3)
```


#### 6.1.8 训练模型并输出结果

最后，使用交叉验证器对训练数据进行训练，得到最优模型。然后用这个模型对测试数据进行预测，并计算出模型的准确率。最终得到的模型准确率为 0.68，这表明模型在预测学历要求方面有一定的效果，但还有提升的空间

```scala
val cvModel = cv.fit(trainingData)
val predictions = cvModel.transform(testData)
val accuracy = evaluator.evaluate(predictions)
println(s"模型准确率: ${accuracy}")
```

![[Pasted image 20251106203411.png|500]]
最终模型的准确率为 0.68，在构建模型的时候，只使用了比较少的列作为特征向量，同时超参数调优的范围也不是很广泛,因为电脑硬件问题没办法将参数调到较高参数，可以在较高的电脑配置中再进行调试

### 6.2 聚类

分析目的 ：将相似职位归类，挖掘出具有共同特征的职位群体，便于求职者更精准地搜索和筛选符合自身条件的职位；同时帮助企业了解市场上的职位竞争格局，优化职位描述和招聘要求，提升招聘效率。

#### 6.2.1 加载数据并进行预处理

```scala
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.types._
import org.apache.spark.ml.feature.{StringIndexer, VectorAssembler}
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.sql.Row

val sqlContext = new SQLContext(sc)

val schema = StructType(Array(
  StructField("id", IntegerType, true),
  StructField("城市", StringType, true),
  StructField("岗位", StringType, true),
  StructField("薪资", DoubleType, true),
  StructField("年限", DoubleType, true),
  StructField("学历要求", StringType, true),
  StructField("公司名称", StringType, true),
  StructField("公司类型", StringType, true),
  StructField("公司规模", IntegerType, true),
  StructField("创建时间", StringType, true)
))

val recruitDF = sqlContext.read.format("com.databricks.spark.csv").schema(schema).option("header", "true").load("hdfs:///recruit/recruit.csv")
```
#### 6.2.2 特征值处理与选择

特征值处理

```scala
val assembler = new VectorAssembler().setInputCols(Array("薪资", "年限", "城市_index", "岗位_index", "学历要求_index", "公司类型_index", "公司规模")).setOutputCol("features")
val finalData = assembler.transform(indexedData)
finalData.select("features").show(5)
```

选择用于聚类的特征

```scala
val assembler = new VectorAssembler().setInputCols(Array("薪资", "年限", "城市_index", "岗位_index", "学历要求_index", "公司类型_index", "公司规模")).setOutputCol("features")
val finalData = assembler.transform(indexedData)
finalData.select("features").show(5)
```

#### 6.2.3 KMeans聚类模型训练

设置KMeans参数，训练模型

```scala
val kmeans = new KMeans().setK(5).setSeed(1L)
val model = kmeans.fit(finalData)
```
#### 6.2.4 查看聚类结果

```scala
val predictions = model.transform(finalData)
predictions.select("id", "prediction").show(30)
```

#### 6.2.5 模型效果评估：聚类误差

WSSSE 是 KMeans 聚类中常用的评估指标，表示所有样本到其所属聚类中心的平方距离之和。数值越小，说明聚类效果越好。

```scala
val cost = model.computeCost(finalData)
println(s"Within Set Sum of Squared Errors (WSSSE) = $cost")
```

#### 6.2.6 超参数调优

使用“肘部法则”，科学地选择最优的 KMeans 聚类数。

```scala
val kValues = Array(2, 3, 4, 5, 6, 7, 8)
var results = Seq[(Int, Double)]()
for (k <- kValues) {
  val kmeans = new KMeans().setK(k).setSeed(1L)
  val model = kmeans.fit(finalData)
  val wssse = model.computeCost(finalData)
  results = results :+ (k, wssse)
  println(s"K = $k, WSSSE = $wssse")
}
val wssseDF = sqlContext.createDataFrame(results).toDF("K", "WSSSE")
wssseDF.show()
```


K = 5 就是该问题最优的 KMeans 聚类

## 7. 数据可视化
### 7.1 开发环境
#### 7.1.1 FastAPI 开发环境

Miniconda 3
Python 3.12
FastAPI 0.115.14
Echarts5

#### 7.1.2 本部分功能

FastAPI 是一个现代化、高性能的 Python Web 框架，专为构建高效、易维护的 API 而设计。框架充分利用 Python 的类型提示系统，结合 Pydantic 实现强大的数据验证和自动序列化，在编写代码时就能捕获大量潜在错误。开发过程中，内置的热重载功能通过简单的 `--reload` 参数实现实时更新，配合详尽的错误提示和调试支持，极大提升了开发效率。

后端需要为前端 Echarts 的可视化提供获取数据的功能接口有 : 
1. 统计每个学历要求出现的次数
2. 统计各岗位在各城市的平均薪资 top5
3. 筛选各城市薪资高于 10k 的职位数
4. 统计不同岗位不同学历不同经验的平均薪资

### 7.2 数据可视化 1
#### 7.2.1 各学历要求的占比

通过饼图可视化“学历词频”可以清晰展示不同教育程度在招聘需求中的占比，帮助快速理解各类学历的分布情况。

#### 7.2.2 项目结构

`/app` 存放后端接口与前端页面的所有代码
	`main.py` 建立后端应用, 编写数据接口与解决跨域问题。
	`/templates` 放前端Echarts可视化的页面。
		education_word_count.html

#### 7.2.3 HTML 模板

```html
<!DOCTYPE html>  
<html style="height: 100%">  
<head>  
    <meta charset="utf-8">  
    <title>学历词频统计</title>  
    <script src="https://cdn.jsdelivr.net/npm/echarts@5/dist/echarts.min.js"></script>  
    <style>        html, body {  
            height: 100%;  
            margin: 0;  
            display: flex;  
            justify-content: center;  
            align-items: center;  
        }  
        #main {  
            width: 800px;  
            height: 600px;  
        }  
    </style>  
</head>  
<body>  
<div id="main"></div>  
<script type="text/javascript">  
    var chartDom = document.getElementById('main');  
    var myChart = echarts.init(chartDom);  
    var option;  
  
    fetch('http://localhost:8000/EducationWordCount')  
        .then(response => response.json())  
        .then(data => {  
            option = {  
                title: {  
                    text: '学历词频统计',  
                    left: 'center'  
                },  
                tooltip: {  
                    trigger: 'item'  
                },  
                legend: {  
                    orient: 'vertical',  
                    left: 'left'  
                },  
                series: [{  
                    name: '学历',  
                    type: 'pie',  
                    radius: '50%',  
                    data: data.map(item => ({value: item.count, name: item.education})),  
                    label: {  
                        formatter: ({name, value, percent}) => `${name}: ${value} (${percent.toFixed(1)}%)`  
                    },  
                    emphasis: {  
                        itemStyle: {  
                            shadowBlur: 10,  
                            shadowOffsetX: 0,  
                            shadowColor: 'rgba(0, 0, 0, 0.5)'  
                        }  
                    }  
                }]  
            };  
  
            option && myChart.setOption(option);  
        });  
</script>  
</body>  
</html>
```

#### 7.2.4 Python 后端代码

```python
from fastapi import FastAPI  
from fastapi.responses import JSONResponse  
from fastapi.responses import FileResponse  
import os  
from fastapi.middleware.cors import CORSMiddleware  
app = FastAPI()  
  
# 允许来自所有来源的跨域请求  
app.add_middleware(  
    CORSMiddleware,  
    allow_origins=["*"],  # 允许所有来源  
    allow_credentials=True,  
    allow_methods=["*"],    # 允许所有方法  
    allow_headers=["*"],   # 允许所有头部  
)  
  
def read_file_data(file_path: str):  
    if os.path.exists(file_path):  
        with open(file_path, 'r') as file:  
            lines = file.readlines()  
            return [line.strip() for line in lines]  
    return []  
  
@app.get("/EducationWordCount")  
def read_education_word_count():  
    file_path = "mr/EducationWordCount/part-r-00000"  
    data = read_file_data(file_path)  
    # 解析: 第一列是学历, 第二列是数量  
    data = [{"education": line.split("\t")[0], "count": int(line.split("\t")[1])} for line in data]  
    return JSONResponse(content=data)

if __name__ == "__main__":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 7.2.5 运行与页面效果展示

![[Pasted image 20250628235834.png]]

![[Pasted image 20250629000309.png]]

- 结论：

### 7.3 数据可视化 2
#### 7.3.1 各岗位在各城市的平均薪资 top5

使用折线图展示不同岗位在各个城市的平均薪资变化趋势，便于观察岗位薪资在不同城市间的波动情况。
- X轴：城市
- Y轴：平均薪资
- 每条折线：代表一个岗位的薪资分布
用于帮助用户直观分析不同岗位在各地的薪资走向和差异。

#### 7.3.2 运行与页面效果展示

![[Pasted image 20250629004936.png]]

- 结论：

### 7.4 数据可视化 3

#### 7.4.1 各城市薪资高于 10k 的职位数

使用地图可视化展示各城市高薪岗位数量的地理分布，颜色深浅反映岗位数量多少，更直观地呈现地域差异。
- 地图区域：城市或省份
- 颜色映射：高薪岗位数量
- 用途：快速识别高薪岗位密集区域
帮助用户从空间维度理解就业市场中薪资分布的地域特征。

#### 5.5.2 运行与页面效果展示

![[Pasted image 20250629012601.png]]


### 7.5 数据可视化 4

#### 7.5.1 不同岗位不同学历不同经验的平均薪资

该功能使用热力图展示岗位薪资与学历、经验年限之间的关系，颜色深浅直观反映薪资水平在不同学历与经验组合下的分布情况。
- X轴：经验年限
- Y轴：学历要求
- 颜色强度：代表薪资高低
帮助用户快速识别高薪岗位在学历与经验维度上的分布规律

#### 7.5.2 运行与页面效果展示

![[Pasted image 20250629010532.png]]


## 8.总结
说明本项目完成的主要功能、体会等

