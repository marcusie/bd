分工 : 
- 刘凯杰：4.2-hive数据分析，5.1-flink流数据实时分析，5-3流数据实时分析与前端数据实时可视化
- 刘照辉：3-数据采集与数据存储，4.1-mapreduce数据分析，7-数据可视化
- 李平颐：4.3-spark-sql数据分析，5.2-spark流数据实时分析，6-高级数据分析

#  招聘信息大数据分析

## 1.项目概述

### 1.1 项目背景

### 1.2 项目功能

### 1.3 关键技术

### 1.4 运行环境

- Linux：Ubuntu 16.04
- Hadoop：2.7.1
- Eclipse：3.8
- Hive：1.2.1
- Flink：1.11
- FastAPI：1.2.1
- Echart：4

## 2.系统架构设计
### 2.1 系统组成

本系统基于伪分布式架构部署，所有组件运行在同一物理节点，逻辑上模拟分布式环境。核心组件包括：
- Hadoop 生态
    - HDFS：分布式文件系统，提供高可靠存储（NameNode + DataNode）
    - YARN：资源调度与管理框架（ResourceManager + NodeManager）
    - MapReduce：分布式计算引擎（JobHistoryServer）
- HBase：基于 HDFS 的分布式列式数据库（HMaster + RegionServer + ZooKeeper）
- Spark：内存计算引擎，支持批处理与流处理（Standalone 模式或 YARN 模式）
- Flink：流批一体计算引擎（Standalone Session Cluster 模式）
- Hive：数据仓库工具，提供 SQL 查询能力（Metastore + HiveServer2）
- 辅助组件
    - ZooKeeper：协调服务（HBase 依赖）
    - MySQL：Hive 元数据存储（替代默认 Derby）

### 2.2 系统协作方式

组件间通过分层协作与服务调用实现数据流转：
1. 存储层
    - HDFS 作为统一存储底座，HBase/Hive/Spark/Flink 均直接读写 HDFS。
    - HBase 依赖 HDFS 存储 HFile，Hive 表数据默认存于 HDFS。
2. 计算层
    - 批处理：
        - MapReduce/Spark/Flink 通过 HDFS 客户端读取数据，计算结果写回 HDFS。
        - Hive 将 SQL 转译为 MapReduce/Spark 作业（需配置 `hive.execution.engine=spark`）。
    - 流处理：
        - Flink 从 Kafka/File 等源读取实时数据（伪分布下可模拟），结果写入 HDFS。
3. 资源调度
    - YARN 统一管理计算资源：Spark/Flink/Hive（MapReduce 模式）均向 YARN 申请容器（Container）。
    - HBase 自主管理 RegionServer 资源，不依赖 YARN。
4. 元数据管理
    - Hive 元数据存储于 MySQL，HBase 依赖 ZooKeeper 维护集群状态。

### 2.3 系统网络拓扑

伪分布式系统采用单节点环回网络，所有服务通过 `localhost`（127.0.0.1）通信：
- 所有组件通过 本地环回地址（127.0.0.1） 通信，端口隔离（如 HDFS 9000、YARN 8088、HBase 16010 等）。
- 无跨节点网络延迟，但资源竞争需通过 YARN 限制（如 CPU/内存配额）。
- 外部客户端通过 `localhost:端口` 访问服务（如 HiveServer2 的 10000 端口）。

![[Pasted image 20251106113817.png]]

## 3.数据采集与数据存储
### 3.1 数据采集

1. 爬取工具：八爪鱼
2. 数据来源：Boss直聘（https://www.zhipin.com/）
3. 原始数据量： 13198
4. 数据字段：城市，关键词。职位，职位详情链接，地区，薪资范围，经验要求，学历要求，公司名称，公司详情链接，公司类型  融资情况，公司规模，标签1，标签2 ，标签3，标签4，标签5，福利待遇

- 原始数据集展示：
![[image-20241108161611427.png]]


### 3.2 数据预处理

#### 3.2.1 去除无用字段、重复行，添加id

```python
import pandas as pd
df = pd.read_csv('recruit-0.csv')
df.drop(columns=['职位详情链接', '公司详情链接'], inplace=True)
df.drop_duplicates(inplace=True)
df.insert(0, 'id', range(1, len(df) + 1))
df.to_csv('recruit-1.csv', index=False)
print("数据处理完成，结果已保存到 'recruit-1.csv'")
```

![[image-20241109174649976.png]]

#### 3.2.2 数据格式化处理

结果：
![[image-20241109175805047.png]]

步骤：

1. 薪资-取中间值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-1.csv', sep=',')
def extract_salary_midpoint(salary_str):
    match = re.match(r'(\d+)-(\d+)K(?:·(\d+)薪)?', salary_str)
    if match:
        lower, upper = int(match.group(1)), int(match.group(2))
        midpoint = (lower + upper) / 2
        return midpoint
    return None
df['薪资中值'] = df['薪资范围'].apply(extract_salary_midpoint)
df = df.drop(columns=['薪资范围'])
df = df.dropna(subset=['薪资中值'])
df.to_csv('recruit-2.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-2.csv'")
```

2. 工作经验值-取最低值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-2.csv', sep=',')
def extract_min_experience(experience_str):
    match = re.match(r'(\d+)-?(\d*)年', experience_str)
    if match:
        min_exp = int(match.group(1))
        return min_exp
    elif experience_str == '1年以内':
        return 0
    elif experience_str == '经验不限':
        return 0
    return None
df['最低经验要求'] = df['经验要求'].apply(extract_min_experience)
df = df.drop(columns=['经验要求'])
df = df.dropna(subset=['最低经验要求'])
df.to_csv('recruit-3.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-3.csv'")
```

3. 公司规模与融资情况处理：合并，规模取中间值
```python
import pandas as pd
import csv
import re
input_file = 'recruit-3.csv'
output_file = 'recruit-4.csv'
def extract_scale(financing_info):
    # 使用正则表达式匹配公司规模
    match = re.search(r'(\d+-\d+|\d+)人|(\d+)以上', financing_info)
    if match:
        return match.group(1) or match.group(2)
    return None
def calculate_midpoint(scale):
    if '-' in scale:
        lower, upper = map(int, scale.split('-'))
        return (lower + upper) // 2
    elif scale.isdigit():
        return int(scale)
    elif '以上' in scale:
        return int(scale.replace('以上', ''))
    else:
        return None
try:
    with open(input_file, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        headers = [field for field in reader.fieldnames if field not in ['公司规模', '融资情况']]
        headers.append('公司规模中值')
        rows = list(reader)
except FileNotFoundError:
    print(f"文件 {input_file} 未找到，请检查文件路径。")
    exit(1)
processed_rows = []
for row in rows:
    scale = row['公司规模'] or extract_scale(row['融资情况'])  # 如果公司规模为空，则从融资情况中提取
    if scale:
        scale = scale.replace('人', '')
        midpoint = calculate_midpoint(scale)
        if midpoint is not None:
            new_row = {key: value for key, value in row.items() if key not in ['公司规模', '融资情况']}
            new_row['公司规模中值'] = midpoint
            processed_rows.append(new_row)
with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.DictWriter(file, fieldnames=headers)
    writer.writeheader()
    writer.writerows(processed_rows)
print(f"处理完成，结果已保存到 {output_file}")
```

#### 3.2.3 多值列表化处理

结果：
![[image-20241109180042964.png]]

步骤：

1. 合并标签
```python
import pandas as pd
df = pd.read_csv('recruit-4.csv', sep=',')
# 定义一个函数来将多列合并为一个列表
def merge_to_list(row):
    return [str(item) for item in row if pd.notnull(item)]
df['标签'] = df[['标签1', '标签2', '标签3', '标签4', '标签5']].apply(merge_to_list, axis=1)
df = df.drop(columns=['标签1', '标签2', '标签3', '标签4', '标签5'])
df.to_csv('recruit-5.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-5.csv'")
```

2. 处理福利待遇转列表
```python
import pandas as pd
import re
df = pd.read_csv('recruit-5.csv', sep=',')
def split_welfare(welfare_str):
    if pd.isnull(welfare_str):
        return []
    return [item.strip() for item in re.split(r'，', welfare_str)]
df['福利待遇列表'] = df['福利待遇'].apply(split_welfare)
df = df.drop(columns=['福利待遇'])
df.to_csv('recruit-6.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-6.csv'")
```

#### 3.2.4 最终结果

![[image-20241109180206673.png]]

### 3.3 数据上传与数据存储

#### 3.3.1 启动并上传数据至Hadoop

启动hadoop
```shell
start-dfs.sh
jps
```
![[image-20250625162057427.png]]
启动成功

数据上传至hdfs
1. 在hdfs中创建存储数据的文件夹
2. 把recruit.csv上传到hdfs的/data/中
3. 查看hdfs中的/data/recruit.csv前十条记录，说明上传成功
```shell
hdfs dfs -mkdir -p /data/recruit
hdfs dfs -put ~/data/recruit.csv /data/recruit
hdfs dfs -cat /data/recruit/recruit.csv | head
```
![[image-20250625163129785.png]]

### 3.4 数据库操作
#### 3.4.1 数据导入

- 启动hbase
```shell
start-hbase.sh
hbase thrift start
hbase shell
count 'recruit'
scan 'recruit', {LIMIT => 5}
```

- 导入数据
```python
python importToHbase.py

import happybase
import csv
def connect_to_hbase():
    connection = happybase.Connection('localhost')
    return connection
def create_table(connection, table_name):
    if bytes(table_name, encoding='utf-8') in connection.tables():
        print(f"Table {table_name} already exists.")
    else:
        families = {
            'info': dict(),  # 列族
        }
        connection.create_table(table_name, families)
        print(f"Created table {table_name}.")
def insert_data_into_hbase(connection, table_name, data):
    table = connection.table(table_name)
    record_count = 0  # 计数器初始化
    with table.batch() as b:
        for row in data:
            row_key = f"{row[0]}".encode('utf-8')
            row_data = {
                'info:city': row[1].encode('utf-8'),
                'info:keyword': row[2].encode('utf-8'),
                'info:position': row[3].encode('utf-8'),
                'info:area': row[4].encode('utf-8'),
                'info:education_requirement': row[5].encode('utf-8'),
                'info:company_name': row[6].encode('utf-8'),
                'info:company_type': row[7].encode('utf-8'),
                'info:salary_median': str(row[8]).encode('utf-8'),  # 将浮点数转换为字符串再编码
                'info:experience_min': str(row[9]).encode('utf-8'),  # 将整数转换为字符串再编码
                'info:company_size_median': str(row[10]).encode('utf-8'),  # 将整数转换为字符串再编码
                'info:tags': row[11].encode('utf-8'),
                'info:benefits': row[12].encode('utf-8')
            }
            b.put(row_key, row_data)
            record_count += 1  # 每次插入后递增计数器
    print(f"Inserted {record_count} records into the table {table_name}.")

def read_csv_and_prepare_data(file_path):
    data = []
    with open(file_path, mode='r', encoding='utf-8') as file:
        reader = csv.reader(file)
        next(reader)  # 跳过标题行
        for row in reader:
            row[8] = float(row[8])  # 薪资中值
            row[9] = int(float(row[9]))  # 最低经验要求
            row[10] = int(float(row[10]))  # 公司规模中值
            data.append(row)
    return data
if __name__ == '__main__':
    conn = connect_to_hbase()
    create_table(conn, 'recruit')
    data = read_csv_and_prepare_data('recruit.csv')
    insert_data_into_hbase(conn, 'recruit', data)
    conn.close()
```

- 验证
```sql
count 'recruit'
```

#### 3.4.2 数据库操作

- 操作菜单         
```python
python hbaseOperate.py

import happybase
import sys

def connect_to_hbase():
    try:
        connection = happybase.Connection('localhost')
        return connection
    except Exception as e:
        print(f"Error connecting to HBase: {e}")
        sys.exit(1)

def create_table(connection, table_name):
    try:
        if bytes(table_name, encoding='utf-8') not in connection.tables():
            families = {'info': dict()}
            connection.create_table(table_name, families)
            print(f"Table {table_name} created.")
        else:
            print(f"Table {table_name} already exists.")
    except Exception as e:
        print(f"Error creating table: {e}")

def fuzzy_query(connection, table_name, column, pattern):
    try:
        table = connection.table(table_name)
        # 使用PrefixFilter实现模糊查询
        rows = table.scan(filter=f"PrefixFilter('{pattern}')")
        for key, data in rows:
            print(f"Row Key: {key.decode('utf-8')}, Column: {column}, Value: {data.get(column.encode('utf-8')).decode('utf-8')}")
    except Exception as e:
        print(f"Error performing fuzzy query: {e}")

def main_menu():
    print("\nHBase Operations Menu:")
    print("1. Insert Data")
    print("2. Update Data")
    print("3. Delete Data")
    print("4. Query by ID")
    print("5. Query by Field")
    print("6. Fuzzy Query")
    print("0. Exit")
    choice = input("Choose an operation (0-6): ")
    return choice

def get_conditions():
    conditions = {}
    while True:
        field = input("Enter field name (without 'info:'), or press Enter to finish: ")
        if not field:
            break
        value = input(f"Enter value to match for field '{field}': ")
        conditions[field] = value
    return conditions

def get_update_fields():
    fields = {}
    while True:
        field = input("Enter field name to update (without 'info:'), or press Enter to finish: ")
        if not field:
            break
        value = input(f"Enter new value for field '{field}': ")
        fields[f'info:{field}'] = value
    return fields

def main():
    conn = connect_to_hbase()
    create_table(conn, 'recruit')

    while True:
        choice = main_menu()
        if choice == '1':
            row_key = input("Enter Row Key: ")
            data_dict = {}
            for i in ['city', 'keyword', 'position', 'area', 'education_requirement', 'company_name', 'company_type', 'salary_median', 'experience_min', 'company_size_median', 'tags', 'benefits']:
                data_dict[f'info:{i}'] = input(f"Enter {i}: ")
            insert_data(conn, 'recruit', row_key, data_dict)
        elif choice == '2':
            row_key = input("Enter Row Key to update: ")
            fields = get_update_fields()
            update_data(conn, 'recruit', row_key, fields)
        elif choice == '3':
            row_key = input("Enter Row Key to delete: ")
            delete_data(conn, 'recruit', row_key)
        elif choice == '4':
            row_key = input("Enter Row Key to query: ")
            query_by_id(conn, 'recruit', row_key)
        elif choice == '5':
            conditions = get_conditions()
            query_by_field(conn, 'recruit', conditions)
        elif choice == '6':
            column = input("Enter column to query (with 'info:'): ")
            pattern = input("Enter prefix pattern: ")
            fuzzy_query(conn, 'recruit', column, pattern)
        elif choice == '0':
            print("Exiting program.")
            break
        else:
            print("Invalid choice, please choose again.")
    
    conn.close()

if __name__ == '__main__':
    main()
```

##### 3.4.2.1 插入数据

```python
def insert_data(connection, table_name, row_key, data_dict):
    try:
        table = connection.table(table_name)
        with table.batch() as b:
            b.put(row_key.encode('utf-8'), {k.encode('utf-8'): str(v).encode('utf-8') for k, v in data_dict.items()})
        print("Data inserted successfully.")
    except Exception as e:
        print(f"Error inserting data: {e}")
```

##### 3.4.2.2 删除数据

```python
def delete_data(connection, table_name, row_key):
    try:
        table = connection.table(table_name)
        table.delete(row_key.encode('utf-8'))
        print("Data deleted successfully.")
    except Exception as e:
        print(f"Error deleting data: {e}")
```

##### 3.4.2.3 修改数据

```python
def update_data(connection, table_name, row_key, data_dict):
    try:
        table = connection.table(table_name)
        with table.batch() as b:
            for field, value in data_dict.items():
                b.put(row_key.encode('utf-8'), {field.encode('utf-8'): str(value).encode('utf-8')})
        print("Data updated successfully.")
    except Exception as e:
        print(f"Error updating data: {e}")
```

##### 3.4.2.4 查询数据

###### 3.4.2.4.1 按城市与公司类型查询

```python
def query_by_field(connection, table_name, conditions):
    try:
        table = connection.table(table_name)
        filters = [f"SingleColumnValueFilter('info', '{field}', =, 'binary:{value}')" for field, value in conditions.items()]
        filter_str = " AND ".join(filters)
        rows = table.scan(filter=filter_str)
        for key, data in rows:
            print(f"Row Key: {key.decode('utf-8')}")
            for col, val in data.items():
                print(f"  {col.decode('utf-8')}: {val.decode('utf-8')}")
    except Exception as e:
        print(f"Error querying by field: {e}")
```

###### 3.4.2.4.2 按城市与岗位关键字与公司类型查询




## 4.大数据统计数据分析

### 4.1 Mapreduce 数据分析
#### 4.1.1 统计每个学历要求出现的次数

- 分析目的：分析企业对求职者的学历要求分布，了解不同岗位的最低学历门槛，帮助求职者评估自身竞争力，同时为企业优化招聘策略提供参考。

- 代码：
```java
package bigdata;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class EducationWordCount {
    public static class EducationMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text education = new Text();
        private Set<String> allowedEducations = new HashSet<>();
        private boolean isHeader = true;

        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            allowedEducations.add("中专/中技");
            allowedEducations.add("初中及以下");
            allowedEducations.add("博士");
            allowedEducations.add("大专");
            allowedEducations.add("学历不限");
            allowedEducations.add("本科");
        }

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 使用更健壮的表头判断方式
            if (isHeader) {
                isHeader = false;
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length > 5) { // 确保有足够的字段
                String educationRequirement = fields[5]; // 学历要求是第六个字段
                if (allowedEducations.contains(educationRequirement)) {
                    education.set(educationRequirement);
                    context.write(education, one);
                }
            }
        }
    }

    public static class EducationReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: EducationWordCount <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Education Word Count");
        job.setJarByClass(EducationWordCount.class);

        // 设置Mapper和Reducer
        job.setMapperClass(EducationMapper.class);
        job.setReducerClass(EducationReducer.class);

        // 设置输出类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 设置输入和输出路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

- 运行情况：
![[image-20250625165440873.png]]
![[image-20250625165502372.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/EducationWordCount
hadoop jar EducationWordCount.jar /data/recruit /data/mr/EducationWordCount
hdfs dfs -ls /data/mr/EducationWordCount
hdfs dfs -cat /data/mr/EducationWordCount/part-r-00000
```

![[image-20250625165513627.png]]

- 结论：


#### 4.1.2 统计各岗位在各城市的平均薪资 top5

- 分析目的：对比不同城市相同岗位的薪资水平，揭示高薪岗位聚集地，帮助求职者选择高薪地区，同时为企业调整薪资竞争力提供数据支持。

- 代码：
```java
package bigdata;

import java.io.IOException;
import java.util.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class JobCitySalaryTop {

    // 第一阶段：计算各岗位在各城市的平均薪资
    public static class SalaryMapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text jobCity = new Text();
        private Text salary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 跳过表头行
            if (key.get() == 0) {
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length >= 4) { // 确保有足够的字段
                try {
                    String city = fields[1];  // 城市在第二列
                    String job = fields[2];   // 岗位在第三列
                    double salaryValue = Double.parseDouble(fields[3]);  // 薪资在第四列
                    
                    jobCity.set(job + "\t" + city);
                    salary.set(String.valueOf(salaryValue));
                    context.write(jobCity, salary);
                } catch (NumberFormatException e) {
                    System.err.println("Invalid salary value: " + value.toString());
                }
            }
        }
    }

    public static class SalaryReducer extends Reducer<Text, Text, Text, Text> {
        private Text job = new Text();
        private Text cityAvgSalary = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            String[] parts = key.toString().split("\t");
            String jobName = parts[0];
            String cityName = parts[1];
            
            double sum = 0;
            int count = 0;
            for (Text val : values) {
                sum += Double.parseDouble(val.toString());
                count++;
            }
            
            double avgSalary = sum / count;
            
            job.set(jobName);
            cityAvgSalary.set(cityName + "\t" + String.format("%.2f", avgSalary));
            context.write(job, cityAvgSalary);
        }
    }

    // 第二阶段：对每个岗位的城市按平均薪资排序并取Top5
    public static class Top5Mapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text job = new Text();
        private Text citySalary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split("\t");
            if (parts.length >= 3) {
                job.set(parts[0]);  // 岗位
                citySalary.set(parts[1] + "\t" + parts[2]);  // 城市和平均薪资
                context.write(job, citySalary);
            }
        }
    }

    public static class Top5Reducer extends Reducer<Text, Text, NullWritable, Text> {
        private Text output = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // 使用TreeMap按平均薪资降序排序
            TreeMap<Double, List<String>> salaryToCities = new TreeMap<>(Collections.reverseOrder());
            
            for (Text val : values) {
                String[] parts = val.toString().split("\t");
                String city = parts[0];
                double salary = Double.parseDouble(parts[1]);
                
                salaryToCities.computeIfAbsent(salary, k -> new ArrayList<>()).add(city);
            }
            
            // 输出Top5
            int topCount = 0;
            for (Map.Entry<Double, List<String>> entry : salaryToCities.entrySet()) {
                double salary = entry.getKey();
                List<String> cities = entry.getValue();
                
                for (String city : cities) {
                    if (topCount >= 5) {
                        break;
                    }
                    
                    output.set(key.toString() + "\t" + city + "\t" + String.format("%.2f", salary));
                    context.write(NullWritable.get(), output);
                    topCount++;
                }
                
                if (topCount >= 5) {
                    break;
                }
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        
        if (otherArgs.length != 3) {
            System.err.println("Usage: JobCitySalaryTop <input path> <temp path> <output path>");
            System.exit(2);
        }

        // 第一阶段作业：计算平均薪资
        Job job1 = Job.getInstance(conf, "JobCitySalaryAvg");
        job1.setJarByClass(JobCitySalaryTop.class);
        
        job1.setMapperClass(SalaryMapper.class);
        job1.setReducerClass(SalaryReducer.class);
        
        job1.setMapOutputKeyClass(Text.class);
        job1.setMapOutputValueClass(Text.class);
        
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job1, new Path(otherArgs[1]));
        
        boolean success = job1.waitForCompletion(true);
        if (!success) {
            System.exit(1);
        }

        // 第二阶段作业：取Top5
        Job job2 = Job.getInstance(conf, "JobCitySalaryTop5");
        job2.setJarByClass(JobCitySalaryTop.class);
        
        job2.setMapperClass(Top5Mapper.class);
        job2.setReducerClass(Top5Reducer.class);
        
        job2.setMapOutputKeyClass(Text.class);
        job2.setMapOutputValueClass(Text.class);
        
        job2.setOutputKeyClass(NullWritable.class);
        job2.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job2, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job2, new Path(otherArgs[2]));
        
        System.exit(job2.waitForCompletion(true) ? 0 : 1);
    }
}
```

- 运行情况：
![[image-20250625230541767.png]]
![[image-20250625230555621.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/JobCitySalaryTop
hadoop jar JobCitySalaryTop.jar /data/recruit /data/temp/JobCitySalaryTop /data/mr/JobCitySalaryTop
hdfs dfs -ls /data/mr/JobCitySalaryTop
hdfs dfs -cat /data/mr/JobCitySalaryTop/part-r-00000
```
![[image-20250625230451123.png]]

- 结论：

#### 4.1.3 筛选各城市薪资高于 10k 的职位数

- 分析目的：衡量各城市高薪岗位的供给情况，反映城市经济活力和行业薪资水平，帮助求职者判断高薪机会分布，同时为企业制定薪酬策略提供参考。

- 代码：
```java
package xyz.hiubo.bigdata;  
  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class CityHighSalaryJobsCount {  
  
    // Mapper：筛选薪资>10k的记录，并按城市分组  
    public static class HighSalaryMapper extends Mapper<LongWritable, Text, Text, LongWritable> {  
        private Text city = new Text();  
        private final LongWritable one = new LongWritable(1);  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 4) { // 确保有薪资字段  
                try {  
                    String cityName = fields[1];  // 城市在第二列  
                    double salary = Double.parseDouble(fields[3]);  // 薪资在第四列  
                    // 筛选薪资>10k的记录  
                    if (salary > 10.0) {  
                        city.set(cityName);  
                        context.write(city, one);  
                    }  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Reducer：统计每个城市的符合条件职位数  
    public static class HighSalaryReducer extends Reducer<Text, LongWritable, Text, LongWritable> {  
        private LongWritable result = new LongWritable();  
  
        @Override  
        protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {  
            long count = 0;  
            for (LongWritable val : values) {  
                count += val.get();  
            }  
            result.set(count);  
            context.write(key, result);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: CityHighSalaryJobsCount <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "City High Salary Jobs Count");  
        job.setJarByClass(CityHighSalaryJobsCount.class);  
          
        job.setMapperClass(HighSalaryMapper.class);  
        job.setReducerClass(HighSalaryReducer.class);  
          
        job.setMapOutputKeyClass(Text.class);  
        job.setMapOutputValueClass(LongWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(LongWritable.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

- 运行情况：
![[image-20250625231431977.png]]
![[image-20250625231442455.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/CityHighSalaryJobsCount
hadoop jar CityHighSalaryJobsCount.jar /data/recruit /data/mr/CityHighSalaryJobsCount
hdfs dfs -ls /data/mr/CityHighSalaryJobsCount
hdfs dfs -cat /data/mr/CityHighSalaryJobsCount/part-r-00000
```
![[image-20250625231454624.png]]

- 结论：

#### 4.1.4 统计不同岗位不同学历不同经验的平均薪资

分析的目的：量化学历、经验对薪资的影响，帮助求职者规划职业发展路径（如是否提升学历或积累经验），同时为企业优化薪酬体系提供数据支撑。

- 代码：
```java
package xyz.hiubo.bigdata;  
  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.*;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class JobEducationExperienceSalaryAvg {  
  
    // 自定义组合键：岗位、学历和经验  
    public static class JobEducationExperienceKey implements WritableComparable<JobEducationExperienceKey> {  
        private Text job;  
        private Text education;  
        private Text experience;  
  
        public JobEducationExperienceKey() {  
            this.job = new Text();  
            this.education = new Text();  
            this.experience = new Text();  
        }  
  
        public JobEducationExperienceKey(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public void set(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public Text getJob() {  
            return job;  
        }  
  
        public Text getEducation() {  
            return education;  
        }  
  
        public Text getExperience() {  
            return experience;  
        }  
  
        @Override  
        public void write(DataOutput out) throws IOException {  
            job.write(out);  
            education.write(out);  
            experience.write(out);  
        }  
  
        @Override  
        public void readFields(DataInput in) throws IOException {  
            job.readFields(in);  
            education.readFields(in);  
            experience.readFields(in);  
        }  
  
        @Override  
        public int compareTo(JobEducationExperienceKey other) {  
            int jobCompare = job.compareTo(other.job);  
            if (jobCompare != 0) {  
                return jobCompare;  
            }  
              
            int educationCompare = education.compareTo(other.education);  
            if (educationCompare != 0) {  
                return educationCompare;  
            }  
              
            return experience.compareTo(other.experience);  
        }  
  
        @Override  
        public int hashCode() {  
            return job.hashCode() * 163 * 163 + education.hashCode() * 163 + experience.hashCode();  
        }  
  
        @Override  
        public boolean equals(Object o) {  
            if (this == o) return true;  
            if (o == null || getClass() != o.getClass()) return false;  
            JobEducationExperienceKey that = (JobEducationExperienceKey) o;  
            return job.equals(that.job) &&   
                   education.equals(that.education) &&   
                   experience.equals(that.experience);  
        }  
  
        @Override  
        public String toString() {  
            return job + "\t" + education + "\t" + experience;  
        }  
    }  
  
    // Mapper：提取岗位、学历、经验和薪资信息  
    public static class SalaryMapper extends Mapper<LongWritable, Text, JobEducationExperienceKey, DoubleWritable> {  
        private JobEducationExperienceKey key = new JobEducationExperienceKey();  
        private DoubleWritable salary = new DoubleWritable();  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 5) { // 确保有足够的字段  
                try {  
                    String job = fields[2];          // 岗位在第三列  
                    String education = fields[5];    // 学历在第六列  
                    String experience = fields[4];   // 经验在第五列  
                    double salaryValue = Double.parseDouble(fields[3]); // 薪资在第四列  
                    this.key.set(new Text(job), new Text(education), new Text(experience));  
                    salary.set(salaryValue);  
                    context.write(this.key, salary);  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Combiner：在Map端进行局部聚合，减少数据传输  
    public static class SalaryCombiner extends Reducer<JobEducationExperienceKey, DoubleWritable, JobEducationExperienceKey, DoubleWritable> {  
        private DoubleWritable result = new DoubleWritable();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            result.set(sum / count);  
            context.write(key, result);  
        }  
    }  
  
    // Reducer：计算平均薪资  
    public static class SalaryReducer extends Reducer<JobEducationExperienceKey, DoubleWritable, Text, Text> {  
        private Text outputKey = new Text();  
        private Text outputValue = new Text();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            double avgSalary = sum / count;  
              
            outputKey.set(key.getJob() + "\t" + key.getEducation() + "\t" + key.getExperience());  
            outputValue.set(String.format("%.2f", avgSalary));  
            context.write(outputKey, outputValue);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: JobEducationExperienceSalaryAvg <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "Job Education Experience Salary Average");  
        job.setJarByClass(JobEducationExperienceSalaryAvg.class);  
          
        job.setMapperClass(SalaryMapper.class);  
        job.setCombinerClass(SalaryCombiner.class);  
        job.setReducerClass(SalaryReducer.class);  
          
        job.setMapOutputKeyClass(JobEducationExperienceKey.class);  
        job.setMapOutputValueClass(DoubleWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(Text.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

- 运行情况：
![[image-20250625232401879.png]]
![[image-20250625232420680.png]]

- 查看结果：
```shell
hdfs dfs -rm -r /data/mr/JobEducationExperienceSalaryAvg
hadoop jar JobEducationExperienceSalaryAvg.jar /data/recruit /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -ls /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -cat /data/mr/JobEducationExperienceSalaryAvg/part-r-00000
```
![[image-20250625232458992.png]]
- 结论：

### 4.2 Hive 数据分析

- 启动 Hive
```shell
hive
```
![[image-20250625234158949.png]]

- 创建 Hive 外部表并验证
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS recruit (
  id INT,
  city STRING,
  position STRING,
  salary DOUBLE,
  experience DOUBLE,
  education STRING,
  company_name STRING,
  company_type STRING,
  company_size INT,
  create_time STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/data/recruit/'
TBLPROPERTIES ("skip.header.line.count"="1");
-- 验证
SELECT * FROM recruit LIMIT 5;
```
![[image-20250625234413525.png]]

#### 4.2.1 统计各城市各岗位的平均薪资 

- 分析目的：构建城市-岗位薪资矩阵，帮助求职者对比不同地区的薪资差异，同时为企业制定区域差异化薪酬政策提供数据支持。

- 代码：
```sql
SELECT 
  position AS position_name,
  city AS city_name,
  percentile_approx(salary, 0.5) AS median_salary
FROM recruit
WHERE salary IS NOT NULL
GROUP BY position, city
ORDER BY position_name, median_salary DESC;
```

- 运行情况：
![[image-20250626161526222.png]]

- 查看结果：
![[image-20250626161606697.png]]

- 结论：

#### 4.2.2 计算不同学历的平均薪资

- 分析目的：量化学历对薪资的影响程度，帮助求职者评估学历提升的投资回报率，同时为企业优化薪酬结构（如学历补贴政策）提供参考。

- 代码：
```sql
SELECT education,AVG(salary) AS avg_salary
	FROM recruit
	GROUP BY education
	ORDER BY avg salary DESC;
```

- 运行情况：
![[image-20250626161651352.png]]

- 查看结果：
![[image-20250626161707064.png]]

- 结论：

#### 4.2.3 按月份统计岗位数量趋势

- 分析目的：分析招聘市场的季节性波动，识别招聘旺季和淡季，帮助求职者把握最佳求职时机，同时为企业优化招聘节奏提供依据。

- 代码：
```sql
SELECT 
    SUBSTR(create_time, 1, 7) AS month,
    COUNT(*) AS post_count
FROM recruit
GROUP BY SUBSTR(create_time, 1, 7)
ORDER BY month;
```

- 运行情况：
![[image-20250626161042697.png]]

- 查看结果：
![[image-20250626161059976.png]]

- 结论：

#### 4.2.4 统计公司类型的分布

- 分析目的：分析不同类型企业（如国企、外企、创业公司）的招聘偏好，帮助求职者选择适合的企业类型，同时为企业研究市场竞争格局提供数据支持。

- 代码：
```sql
SELECT 
    company_type,
    COUNT(*) AS total_positions,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage
FROM recruit
GROUP BY company_type
ORDER BY total_positions DESC;
```

- 运行情况：
![[image-20250626161222800.png]]

- 查看结果：
![[image-20250626161242867.png]]

- 结论：

### 4.3 Spark SQL 数据分析
#### 4.3.1

- 分析目的：

- 代码：

- 运行情况：

- 查看结果：

- 结论：

#### 4.3.2

- 分析目的：

- 代码：

- 运行情况：

- 查看结果：

- 结论：

#### 4.3.3

- 分析目的：

- 代码：

- 运行情况：

- 查看结果：

- 结论：

#### 4.3.4

- 分析目的：

- 代码：

- 运行情况：

- 查看结果：

- 结论：


## 5.实时数据分析

### 5.1 Flink 流数据实时分析
#### 5.1.1 实时统计城市岗位数量

- 分析目的：动态监测各城市的招聘需求变化，帮助求职者快速捕捉最新机会，同时为企业实时调整招聘策略（如区域扩张或收缩）提供决策依据。

- 代码：
```Java
package xyz.hiubo.bigdata;  
  
import org.apache.flink.api.common.eventtime.WatermarkStrategy;  
import org.apache.flink.api.common.functions.FlatMapFunction;  
import org.apache.flink.api.java.tuple.Tuple3;  
import org.apache.flink.configuration.Configuration;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.connector.file.src.FileSource;  
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;  
import org.apache.flink.core.fs.Path;  
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;  
import org.apache.flink.streaming.api.windowing.time.Time;  
import org.apache.flink.util.Collector;  
  
import java.time.Duration;  
  
public class CityJobCount {  
  
    public static void main(String[] args) throws Exception {  
  
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
        env.setParallelism(1);  
  
        // 啟用 checkpoint 以支持持續監聽新文件  
        env.enableCheckpointing(10000);  
  
        // 使用 TextLineInputFormat 按行讀取文本文件  
        FileSource<String> fileSource = FileSource.forRecordStreamFormat(  
                        new TextLineInputFormat(),  
                        new Path("/home/hiubo/data/recruit/stream1")  
                )  
                .monitorContinuously(Duration.ofSeconds(5))  
                .build();  
  
        DataStream<String> lines = env.fromSource(fileSource, WatermarkStrategy.noWatermarks(), "File Source");  
  
        // 解析每行為 (city, job, count)        DataStream<Tuple3<String, String, Integer>> parsed = lines.flatMap(  
                new FlatMapFunction<String, Tuple3<String, String, Integer>>() {  
                    @Override  
                    public void flatMap(String line, Collector<Tuple3<String, String, Integer>> out) {  
                        String[] parts = line.split(",");  
                        if (parts.length == 2) {  
                            String city = parts[0].trim();  
                            String job = parts[1].trim();  
                            out.collect(Tuple3.of(city, job, 1));  
                        }  
                    }  
                }  
        );  
  
        // 按照 (city, job) 分組並統計數量  
        parsed.keyBy(value -> value.f0 + ":" + value.f1)  
                .sum(2)  
                .print();  
  
        env.execute("City Job Count Streaming");  
    }  
}
```

- 运行情况：
![[Pasted image 20250628174143.png]]

![[Pasted image 20250628174208.png]]

![[Pasted image 20250628174227.png]]

- 结论：


#### 5.1.2 实时统计行业分布

- 分析目的：对最新招聘数据进行实时处理和监控，及时掌握各行业人才需求变化趋势，为行业分析和市场预测提供最及时的 数据支持；确保招聘平台能够第一时间根据行业动态调整推荐策略，满足不同行业的人才招聘需求。

- 消费者：
```java
package xyz.hiubo.bigdata;

import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.util.Collector;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import java.nio.charset.StandardCharsets;
import java.util.Properties;

public class FlinkKafkaJobStats {
    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "flink-consumer-group");
        properties.setProperty("auto.offset.reset", "earliest");

        // 自定义 Kafka Deserialization Schema
        KafkaDeserializationSchema<String> deserializer = new KafkaDeserializationSchema<String>() {
            @Override
            public boolean isEndOfStream(String nextElement) {
                return false;
            }

            @Override
            public String deserialize(ConsumerRecord<byte[], byte[]> record) {
                if (record.value() == null) {
                    return null;
                }
                return new String(record.value(), StandardCharsets.UTF_8);
            }

            @Override
            public TypeInformation<String> getProducedType() {
                return TypeInformation.of(String.class);
            }
        };

        DataStream<String> kafkaStream = env.addSource(
                new FlinkKafkaConsumer<>("industry-job", deserializer, properties)
        );

        DataStream<Tuple2<String, Integer>> parsedStream = kafkaStream
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
                    @Override
                    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {
                        String industry = value.trim();
                        if (!industry.isEmpty()) {
                            out.collect(Tuple2.of(industry, 1));
                        }
                    }
                });

        parsedStream.keyBy(value -> value.f0)
                .timeWindow(Time.seconds(5))
                .sum(1)
                .print();

        env.execute("Flink Kafka Industry Job Count");
    }
}
```

- 生产者：
```java
package xyz.hiubo.bigdata;

import org.apache.kafka.clients.producer.*;
import com.opencsv.CSVReader;
import java.io.FileReader;
import java.util.Properties;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

public class KafkaCsvProducer {

    private static final String TOPIC = "industry-job";
    private static final String CSV_FILE_PATH = "/home/hiubo/data/recruit/industry_job.csv";

    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");

        Producer<String, String> producer = new KafkaProducer<>(props);

        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);
        scheduler.scheduleAtFixedRate(() -> {
            try (CSVReader reader = new CSVReader(new FileReader(CSV_FILE_PATH))) {
                int count = 0;
                String[] nextLine;
                while ((nextLine = reader.readNext()) != null && count++ < 1000) {
                    String industry = nextLine[0]; // 第一列是行业
                    producer.send(new ProducerRecord<>(TOPIC, industry));
                }
                System.out.println("Sent 1000 records to Kafka.");
            } catch (Exception e) {
                e.printStackTrace();
            }
        }, 0, 5, TimeUnit.SECONDS);
    }
}
```

- 运行情况：
![[Pasted image 20250628220809.png|204]]

![[Pasted image 20250628215935.png|135]]

![[Pasted image 20250628215949.png|160]]

![[Pasted image 20250628215958.png|160]]

- 结论：

### 5.2 Spark 流数据实时分析
#### 5.2.1

- 分析目的：

- 代码：

- 运行情况：

- 结论：

#### 5.2.2

- 分析目的：

- 代码：

- 运行情况：

- 结论：

### 5.3 流数据实时分析与前端数据实时可视化

- 分析目的：

- 代码：

- 运行情况：

- 结论：

## 6.高级数据分析

### 6.1 分类

- 功能

- 算法

- 分析过程

- 模型代码

- 运行过程与结果

- 模型评估

- 结论

### 6.2 聚类
- 功能

- 算法

- 分析过程

- 模型代码

- 运行过程与结果

- 模型评估

- 结论

## 7. 数据可视化
### 7.1 开发环境
#### 7.1.1 FastAPI 开发环境

Miniconda 3
Python 3.12
FastAPI 0.115.14
Echarts5

#### 7.1.2 本部分功能

FastAPI 是一个现代化、高性能的 Python Web 框架，专为构建高效、易维护的 API 而设计。框架充分利用 Python 的类型提示系统，结合 Pydantic 实现强大的数据验证和自动序列化，在编写代码时就能捕获大量潜在错误。开发过程中，内置的热重载功能通过简单的 `--reload` 参数实现实时更新，配合详尽的错误提示和调试支持，极大提升了开发效率。

后端需要为前端 Echarts 的可视化提供获取数据的功能接口有 : 
1. 统计每个学历要求出现的次数
2. 统计各岗位在各城市的平均薪资 top5
3. 筛选各城市薪资高于 10k 的职位数
4. 统计不同岗位不同学历不同经验的平均薪资

### 7.2 数据可视化 1
#### 7.2.1 各学历要求的占比

通过饼图可视化“学历词频”可以清晰展示不同教育程度在招聘需求中的占比，帮助快速理解各类学历的分布情况。

#### 7.2.2 项目结构

`/app` 存放后端接口与前端页面的所有代码
	`main.py` 建立后端应用, 编写数据接口与解决跨域问题。
	`/templates` 放前端Echarts可视化的页面。
		education_word_count.html

#### 7.2.3 HTML 模板

```html
<!DOCTYPE html>  
<html style="height: 100%">  
<head>  
    <meta charset="utf-8">  
    <title>学历词频统计</title>  
    <script src="https://cdn.jsdelivr.net/npm/echarts@5/dist/echarts.min.js"></script>  
    <style>        html, body {  
            height: 100%;  
            margin: 0;  
            display: flex;  
            justify-content: center;  
            align-items: center;  
        }  
        #main {  
            width: 800px;  
            height: 600px;  
        }  
    </style>  
</head>  
<body>  
<div id="main"></div>  
<script type="text/javascript">  
    var chartDom = document.getElementById('main');  
    var myChart = echarts.init(chartDom);  
    var option;  
  
    fetch('http://localhost:8000/EducationWordCount')  
        .then(response => response.json())  
        .then(data => {  
            option = {  
                title: {  
                    text: '学历词频统计',  
                    left: 'center'  
                },  
                tooltip: {  
                    trigger: 'item'  
                },  
                legend: {  
                    orient: 'vertical',  
                    left: 'left'  
                },  
                series: [{  
                    name: '学历',  
                    type: 'pie',  
                    radius: '50%',  
                    data: data.map(item => ({value: item.count, name: item.education})),  
                    label: {  
                        formatter: ({name, value, percent}) => `${name}: ${value} (${percent.toFixed(1)}%)`  
                    },  
                    emphasis: {  
                        itemStyle: {  
                            shadowBlur: 10,  
                            shadowOffsetX: 0,  
                            shadowColor: 'rgba(0, 0, 0, 0.5)'  
                        }  
                    }  
                }]  
            };  
  
            option && myChart.setOption(option);  
        });  
</script>  
</body>  
</html>
```

#### 7.2.4 Python 后端代码

```python
from fastapi import FastAPI  
from fastapi.responses import JSONResponse  
from fastapi.responses import FileResponse  
import os  
from fastapi.middleware.cors import CORSMiddleware  
app = FastAPI()  
  
# 允许来自所有来源的跨域请求  
app.add_middleware(  
    CORSMiddleware,  
    allow_origins=["*"],  # 允许所有来源  
    allow_credentials=True,  
    allow_methods=["*"],    # 允许所有方法  
    allow_headers=["*"],   # 允许所有头部  
)  
  
def read_file_data(file_path: str):  
    if os.path.exists(file_path):  
        with open(file_path, 'r') as file:  
            lines = file.readlines()  
            return [line.strip() for line in lines]  
    return []  
  
@app.get("/EducationWordCount")  
def read_education_word_count():  
    file_path = "mr/EducationWordCount/part-r-00000"  
    data = read_file_data(file_path)  
    # 解析: 第一列是学历, 第二列是数量  
    data = [{"education": line.split("\t")[0], "count": int(line.split("\t")[1])} for line in data]  
    return JSONResponse(content=data)

if __name__ == "__main__":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 7.2.5 运行与页面效果展示

![[Pasted image 20250628235834.png]]

![[Pasted image 20250629000309.png]]

- 结论：

### 7.3 数据可视化 2
#### 7.3.1 各岗位在各城市的平均薪资 top5

使用折线图展示不同岗位在各个城市的平均薪资变化趋势，便于观察岗位薪资在不同城市间的波动情况。
- X轴：城市
- Y轴：平均薪资
- 每条折线：代表一个岗位的薪资分布
用于帮助用户直观分析不同岗位在各地的薪资走向和差异。

#### 7.3.2 运行与页面效果展示

![[Pasted image 20250629004936.png]]

- 结论：

### 7.4 数据可视化 3

#### 7.4.1 各城市薪资高于 10k 的职位数

使用地图可视化展示各城市高薪岗位数量的地理分布，颜色深浅反映岗位数量多少，更直观地呈现地域差异。
- 地图区域：城市或省份
- 颜色映射：高薪岗位数量
- 用途：快速识别高薪岗位密集区域
帮助用户从空间维度理解就业市场中薪资分布的地域特征。

#### 5.5.2 运行与页面效果展示

![[Pasted image 20250629012601.png]]


### 7.5 数据可视化 4

#### 7.5.1 不同岗位不同学历不同经验的平均薪资

该功能使用热力图展示岗位薪资与学历、经验年限之间的关系，颜色深浅直观反映薪资水平在不同学历与经验组合下的分布情况。
- X轴：经验年限
- Y轴：学历要求
- 颜色强度：代表薪资高低
帮助用户快速识别高薪岗位在学历与经验维度上的分布规律

#### 7.5.2 运行与页面效果展示

![[Pasted image 20250629010532.png]]


## 8.总结
说明本项目完成的主要功能、体会等

