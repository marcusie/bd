#  招聘信息大数据分析

## 1. 项目背景与功能

### 1.1 项目背景

随着信息技术的快速发展和数字经济的不断壮大，软件开发、互联网服务以及新兴技术领域（如人工智能、大数据、云计算 等）的需求日益增长。这一趋势不仅推动了整个行业的扩张，同时也对专业人才提出了更高的要求。程序员作为信息时代的 核心力量，在推动技术创新和实现业务目标方面发挥着至关重要的作用。 
近年来，随着企业对技术重视程度的提升，程序员相关职位的需求呈现出爆发式增长。不同规模的企业，从初创公司到大型 跨国集团，都在积极寻找具备特定技能和技术背景的编程人才。然而，面对快速变化的技术环境和激烈的市场竞争，如何准 确把握市场需求、合理评估人才价值、有效配置人力资源成为了企业和求职者共同面临的挑战。 
在此背景下，开展一项关于程序员相关招聘岗位的市场分析与趋势预测的研究具有重要意义。

![[Pasted image 20250628180846.png]]

本项目构建了一套基于大数据技术的IT行业招聘数据分析平台，整合了HDFS分布式存储、MapReduce批处理、Hive数据仓库、Flink流批一体计算以及FastAPI+ECharts可视化等技术组件。系统采用分层架构设计，首先通过HDFS实现海量招聘数据的可靠存储，采用三副本机制确保数据安全，并支持多种数据格式的读写操作。在数据处理层，利用MapReduce进行离线数据清洗和基础统计分析，实现了包括岗位数量统计、薪资区间分布、技能词频分析等核心功能；同时通过Hive构建数据仓库，使用HQL实现复杂的数据聚合查询和多维度分析，并与HBase集成实现结构化与非结构化数据的联合查询。在实时计算方面，基于Flink框架实现了流批一体的处理能力，既支持历史数据的批量分析，又能实时处理新发布的招聘信息，通过事件时间语义和滑动窗口计算等技术，实时追踪热门技术趋势变化和区域薪资波动。最后，通过FastAPI构建高性能的RESTful API服务，结合ECharts丰富的可视化图表库，实现了包括动态热力图、实时折线图、多维雷达图等多种可视化展示，支持用户交互式探索数据。该平台不仅为IT企业提供了精准的人才市场分析，也为求职者把握行业趋势提供了数据支持，充分体现了大数据技术在行业分析中的应用价值。

### 1.2 项目功能

1. 数据爬取：从网站上爬取原始数据； 
2. 数据预处理：对爬取的数据进行清洗、去重、格式转换等预处理操作； 
3. 数据上传至 HDFS：将预处理后的数据上传至 HDFS中； 
4. 使用 MapReduce 编程进行统计分析：频率统计、分区、排序、最值、TopN 、全排序、 二次排 序、 自定义类、各类比例、多个字段组合统计等功能； 
5. 数据上传至 Hive：将预处理后的数据上传至 Hive 数据库中； 
6. 使用 Hive 进行统计分析：排序、聚类函数、分组统计等操作； 
7. 使用 Flink 的批处理和流处理功能万册数据分析；
8. 使用 FastAPI 和 Echart 完成数据分析结果的可视化功能；

### 1.3 运行环境

Linux: Ubuntu 16.04，Hadoop: 2.7.1，Eclipse: 3.8，Hive：1.2.1，Flink：1.11，FastAPI：1.2.1，Echart：1.2.1

## 2. 数据集与数据预处理

### 2.1 原始数据集

#### 2.1.1 数据集说明

1. 爬取工具：八爪鱼
2. 数据来源：Boss直聘（https://www.zhipin.com/）
3. 原始数据量： 13198
4. 数据字段：城市，关键词。职位，职位详情链接，地区，薪资范围，经验要求，学历要求，公司名称，公司详情链接，公司类型  融资情况，公司规模，标签1，标签2 ，标签3，标签4，标签5，福利待遇

#### 2.1.2 原始数据集展示

![[1-trash/0-大数据/工程实践/-assets/image-20241108161611427.png]]


### 2.2 数据预处理

#### 2.2.1 去除无用字段、重复行，添加id

```python
import pandas as pd
df = pd.read_csv('recruit-0.csv')
df.drop(columns=['职位详情链接', '公司详情链接'], inplace=True)
df.drop_duplicates(inplace=True)
df.insert(0, 'id', range(1, len(df) + 1))
df.to_csv('recruit-1.csv', index=False)
print("数据处理完成，结果已保存到 'recruit-1.csv'")
```

![[1-trash/0-大数据/工程实践/-assets/image-20241109174649976.png]]

#### 2.2.2 数据处理

结果：
![[1-trash/0-大数据/工程实践/-assets/image-20241109175805047.png]]

步骤：
1. 薪资-取中间值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-1.csv', sep=',')
def extract_salary_midpoint(salary_str):
    match = re.match(r'(\d+)-(\d+)K(?:·(\d+)薪)?', salary_str)
    if match:
        lower, upper = int(match.group(1)), int(match.group(2))
        midpoint = (lower + upper) / 2
        return midpoint
    return None
df['薪资中值'] = df['薪资范围'].apply(extract_salary_midpoint)
df = df.drop(columns=['薪资范围'])
df = df.dropna(subset=['薪资中值'])
df.to_csv('recruit-2.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-2.csv'")
```

2. 工作经验值-取最低值
```python
import pandas as pd
import re
df = pd.read_csv('recruit-2.csv', sep=',')
def extract_min_experience(experience_str):
    match = re.match(r'(\d+)-?(\d*)年', experience_str)
    if match:
        min_exp = int(match.group(1))
        return min_exp
    elif experience_str == '1年以内':
        return 0
    elif experience_str == '经验不限':
        return 0
    return None
df['最低经验要求'] = df['经验要求'].apply(extract_min_experience)
df = df.drop(columns=['经验要求'])
df = df.dropna(subset=['最低经验要求'])
df.to_csv('recruit-3.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-3.csv'")
```

3. 公司规模与融资情况处理：合并，规模取中间值
```python
import pandas as pd
import csv
import re
input_file = 'recruit-3.csv'
output_file = 'recruit-4.csv'
def extract_scale(financing_info):
    # 使用正则表达式匹配公司规模
    match = re.search(r'(\d+-\d+|\d+)人|(\d+)以上', financing_info)
    if match:
        return match.group(1) or match.group(2)
    return None
def calculate_midpoint(scale):
    if '-' in scale:
        lower, upper = map(int, scale.split('-'))
        return (lower + upper) // 2
    elif scale.isdigit():
        return int(scale)
    elif '以上' in scale:
        return int(scale.replace('以上', ''))
    else:
        return None
try:
    with open(input_file, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        headers = [field for field in reader.fieldnames if field not in ['公司规模', '融资情况']]
        headers.append('公司规模中值')
        rows = list(reader)
except FileNotFoundError:
    print(f"文件 {input_file} 未找到，请检查文件路径。")
    exit(1)
processed_rows = []
for row in rows:
    scale = row['公司规模'] or extract_scale(row['融资情况'])  # 如果公司规模为空，则从融资情况中提取
    if scale:
        scale = scale.replace('人', '')
        midpoint = calculate_midpoint(scale)
        if midpoint is not None:
            new_row = {key: value for key, value in row.items() if key not in ['公司规模', '融资情况']}
            new_row['公司规模中值'] = midpoint
            processed_rows.append(new_row)
with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.DictWriter(file, fieldnames=headers)
    writer.writeheader()
    writer.writerows(processed_rows)
print(f"处理完成，结果已保存到 {output_file}")
```

#### 2.2.3 合并标签与处理福利待遇

结果：
![[1-trash/0-大数据/工程实践/-assets/image-20241109180042964.png]]

步骤：
1. 合并标签
```python
import pandas as pd
df = pd.read_csv('recruit-4.csv', sep=',')
# 定义一个函数来将多列合并为一个列表
def merge_to_list(row):
    return [str(item) for item in row if pd.notnull(item)]
df['标签'] = df[['标签1', '标签2', '标签3', '标签4', '标签5']].apply(merge_to_list, axis=1)
df = df.drop(columns=['标签1', '标签2', '标签3', '标签4', '标签5'])
df.to_csv('recruit-5.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-5.csv'")
```

2. 处理福利待遇转列表
```python
import pandas as pd
import re
df = pd.read_csv('recruit-5.csv', sep=',')
def split_welfare(welfare_str):
    if pd.isnull(welfare_str):
        return []
    return [item.strip() for item in re.split(r'，', welfare_str)]
df['福利待遇列表'] = df['福利待遇'].apply(split_welfare)
df = df.drop(columns=['福利待遇'])
df.to_csv('recruit-6.csv', index=False, sep=',')
print("数据处理完成，结果已保存到 'recruit-6.csv'")
```

#### 2.2.4 最终结果

![[1-trash/0-大数据/工程实践/-assets/image-20241109180206673.png]]

### 2.3 数据上传

#### 2.3.1 启动Hadoop

```shell
start-dfs.sh
jps
```

![[image-20250625162057427.png]]

#### 2.3.2 上传数据至HDFS

1. 在HDFS中创建存储数据的文件夹
2. 把recruit.csv上传到HDFS的/lzh/中
3. 查看HDFS中的/lzh/recruit.csv前十条记录，说明上传成功

```shell
hdfs dfs -mkdir -p /data/recruit
hdfs dfs -put ~/data/recruit.csv /data/recruit
hdfs dfs -cat /data/recruit/recruit.csv | head
```

![[image-20250625163129785.png]]

## 3.数据分析

### 3.1 MapReduce 数据分析

启动 yarn
```Shell
start-yarn.sh
jps
cd /home/hiubo/developer/project/code/bigdata-demo/bin/xyz/hiubo/bigdata/
```

![[image-20250625163501525.png]]

#### 3.1.1 统计每个学历要求出现的次数

分析的目的：分析企业对求职者的学历要求分布，了解不同岗位的最低学历门槛，帮助求职者评估自身竞争力，同时为企业优化招聘策略提供参考。

```java
package bigdata;

import java.io.IOException;
import java.util.HashSet;
import java.util.Set;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class EducationWordCount {
    public static class EducationMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text education = new Text();
        private Set<String> allowedEducations = new HashSet<>();
        private boolean isHeader = true;

        @Override
        protected void setup(Context context) throws IOException, InterruptedException {
            allowedEducations.add("中专/中技");
            allowedEducations.add("初中及以下");
            allowedEducations.add("博士");
            allowedEducations.add("大专");
            allowedEducations.add("学历不限");
            allowedEducations.add("本科");
        }

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 使用更健壮的表头判断方式
            if (isHeader) {
                isHeader = false;
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length > 5) { // 确保有足够的字段
                String educationRequirement = fields[5]; // 学历要求是第六个字段
                if (allowedEducations.contains(educationRequirement)) {
                    education.set(educationRequirement);
                    context.write(education, one);
                }
            }
        }
    }

    public static class EducationReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        @Override
        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            context.write(key, new IntWritable(sum));
        }
    }

    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: EducationWordCount <input path> <output path>");
            System.exit(-1);
        }

        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "Education Word Count");
        job.setJarByClass(EducationWordCount.class);

        // 设置Mapper和Reducer
        job.setMapperClass(EducationMapper.class);
        job.setReducerClass(EducationReducer.class);

        // 设置输出类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 设置输入和输出路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

![[image-20250625165440873.png]]
![[image-20250625165502372.png]]

```shell
hdfs dfs -rm -r /data/mr/EducationWordCount
hadoop jar EducationWordCount.jar /data/recruit /data/mr/EducationWordCount
hdfs dfs -ls /data/mr/EducationWordCount
hdfs dfs -cat /data/mr/EducationWordCount/part-r-00000
```

![[image-20250625165513627.png]]

#### 3.1.2 统计各岗位在各城市的招聘数量 top5

分析的目的：识别不同城市的热门岗位需求，反映区域产业发展特点，帮助求职者选择就业方向，同时为企业制定区域招聘计划提供依据。

```java
package bigdata;

import java.io.IOException;
import java.util.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class JobCityTop {

    // 第一阶段：统计各岗位在各城市的招聘数量
    public static class CountMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
        private Text jobCity = new Text();
        private final LongWritable one = new LongWritable(1);

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 跳过表头
            if (key.get() == 0) {
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length >= 3) { // 确保有足够的字段
                String job = fields[2]; // 假设岗位是第三列
                String city = fields[1]; // 假设城市是第二列
                jobCity.set(job + "\t" + city);
                context.write(jobCity, one);
            }
        }
    }

    public static class CountReducer extends Reducer<Text, LongWritable, Text, Text> {
        private Text job = new Text();
        private Text cityCount = new Text();

        @Override
        protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {
            String[] parts = key.toString().split("\t");
            String jobName = parts[0];
            String cityName = parts[1];
            
            long count = 0;
            for (LongWritable val : values) {
                count += val.get();
            }
            
            job.set(jobName);
            cityCount.set(cityName + "\t" + count);
            context.write(job, cityCount);
        }
    }

    // 第二阶段：对每个岗位的城市进行排序并取Top5
    public static class Top5Mapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text job = new Text();
        private Text cityCount = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split("\t");
            if (parts.length >= 3) {
                job.set(parts[0]); // 岗位
                cityCount.set(parts[1] + "\t" + parts[2]); // 城市+数量
                context.write(job, cityCount);
            }
        }
    }

    public static class Top5Reducer extends Reducer<Text, Text, NullWritable, Text> {
        private Text output = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // 使用TreeMap按数量降序排序
            TreeMap<Long, List<String>> countToCities = new TreeMap<>(Collections.reverseOrder());
            
            for (Text val : values) {
                String[] parts = val.toString().split("\t");
                String city = parts[0];
                long count = Long.parseLong(parts[1]);
                
                countToCities.computeIfAbsent(count, k -> new ArrayList<>()).add(city);
            }
            
            // 输出Top5
            int topCount = 0;
            for (Map.Entry<Long, List<String>> entry : countToCities.entrySet()) {
                long count = entry.getKey();
                List<String> cities = entry.getValue();
                
                for (String city : cities) {
                    if (topCount >= 5) {
                        break;
                    }
                    
                    output.set(key.toString() + "\t" + city + "\t" + count);
                    context.write(NullWritable.get(), output);
                    topCount++;
                }
                
                if (topCount >= 5) {
                    break;
                }
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        
        if (otherArgs.length != 3) {
            System.err.println("Usage: JobCityTop <input path> <temp path> <output path>");
            System.exit(2);
        }

        // 第一阶段作业：统计数量
        Job job1 = Job.getInstance(conf, "JobCityCount");
        job1.setJarByClass(JobCityTop.class);
        
        job1.setMapperClass(CountMapper.class);
        job1.setReducerClass(CountReducer.class);
        
        job1.setMapOutputKeyClass(Text.class);
        job1.setMapOutputValueClass(LongWritable.class);
        
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job1, new Path(otherArgs[1]));
        
        boolean success = job1.waitForCompletion(true);
        if (!success) {
            System.exit(1);
        }

        // 第二阶段作业：取Top5
        Job job2 = Job.getInstance(conf, "JobCityTop5");
        job2.setJarByClass(JobCityTop.class);
        
        job2.setMapperClass(Top5Mapper.class);
        job2.setReducerClass(Top5Reducer.class);
        
        job2.setMapOutputKeyClass(Text.class);
        job2.setMapOutputValueClass(Text.class);
        
        job2.setOutputKeyClass(NullWritable.class);
        job2.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job2, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job2, new Path(otherArgs[2]));
        
        System.exit(job2.waitForCompletion(true) ? 0 : 1);
    }
}
```

![[image-20250625171134976.png]]

![[image-20250625171151591.png]]

```Shell
hdfs dfs -rm -r /data/mr/JobCityTop
hadoop jar JobCityTop.jar /data/recruit /data/temp/JobCityTop /data/mr/JobCityTop
hdfs dfs -ls /data/mr/JobCityTop
hdfs dfs -cat /data/mr/JobCityTop/part-r-00000
```

![[image-20250625171039217.png]]

#### 3.1.3 统计各岗位在各城市的平均薪资 top5

分析的目的：对比不同城市相同岗位的薪资水平，揭示高薪岗位聚集地，帮助求职者选择高薪地区，同时为企业调整薪资竞争力提供数据支持。

```java
package bigdata;

import java.io.IOException;
import java.util.*;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.*;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class JobCitySalaryTop {

    // 第一阶段：计算各岗位在各城市的平均薪资
    public static class SalaryMapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text jobCity = new Text();
        private Text salary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            // 跳过表头行
            if (key.get() == 0) {
                return;
            }
            
            String[] fields = value.toString().split(",");
            if (fields.length >= 4) { // 确保有足够的字段
                try {
                    String city = fields[1];  // 城市在第二列
                    String job = fields[2];   // 岗位在第三列
                    double salaryValue = Double.parseDouble(fields[3]);  // 薪资在第四列
                    
                    jobCity.set(job + "\t" + city);
                    salary.set(String.valueOf(salaryValue));
                    context.write(jobCity, salary);
                } catch (NumberFormatException e) {
                    System.err.println("Invalid salary value: " + value.toString());
                }
            }
        }
    }

    public static class SalaryReducer extends Reducer<Text, Text, Text, Text> {
        private Text job = new Text();
        private Text cityAvgSalary = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            String[] parts = key.toString().split("\t");
            String jobName = parts[0];
            String cityName = parts[1];
            
            double sum = 0;
            int count = 0;
            for (Text val : values) {
                sum += Double.parseDouble(val.toString());
                count++;
            }
            
            double avgSalary = sum / count;
            
            job.set(jobName);
            cityAvgSalary.set(cityName + "\t" + String.format("%.2f", avgSalary));
            context.write(job, cityAvgSalary);
        }
    }

    // 第二阶段：对每个岗位的城市按平均薪资排序并取Top5
    public static class Top5Mapper extends Mapper<LongWritable, Text, Text, Text> {
        private Text job = new Text();
        private Text citySalary = new Text();

        @Override
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
            String[] parts = value.toString().split("\t");
            if (parts.length >= 3) {
                job.set(parts[0]);  // 岗位
                citySalary.set(parts[1] + "\t" + parts[2]);  // 城市和平均薪资
                context.write(job, citySalary);
            }
        }
    }

    public static class Top5Reducer extends Reducer<Text, Text, NullWritable, Text> {
        private Text output = new Text();

        @Override
        protected void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            // 使用TreeMap按平均薪资降序排序
            TreeMap<Double, List<String>> salaryToCities = new TreeMap<>(Collections.reverseOrder());
            
            for (Text val : values) {
                String[] parts = val.toString().split("\t");
                String city = parts[0];
                double salary = Double.parseDouble(parts[1]);
                
                salaryToCities.computeIfAbsent(salary, k -> new ArrayList<>()).add(city);
            }
            
            // 输出Top5
            int topCount = 0;
            for (Map.Entry<Double, List<String>> entry : salaryToCities.entrySet()) {
                double salary = entry.getKey();
                List<String> cities = entry.getValue();
                
                for (String city : cities) {
                    if (topCount >= 5) {
                        break;
                    }
                    
                    output.set(key.toString() + "\t" + city + "\t" + String.format("%.2f", salary));
                    context.write(NullWritable.get(), output);
                    topCount++;
                }
                
                if (topCount >= 5) {
                    break;
                }
            }
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
        
        if (otherArgs.length != 3) {
            System.err.println("Usage: JobCitySalaryTop <input path> <temp path> <output path>");
            System.exit(2);
        }

        // 第一阶段作业：计算平均薪资
        Job job1 = Job.getInstance(conf, "JobCitySalaryAvg");
        job1.setJarByClass(JobCitySalaryTop.class);
        
        job1.setMapperClass(SalaryMapper.class);
        job1.setReducerClass(SalaryReducer.class);
        
        job1.setMapOutputKeyClass(Text.class);
        job1.setMapOutputValueClass(Text.class);
        
        job1.setOutputKeyClass(Text.class);
        job1.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job1, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job1, new Path(otherArgs[1]));
        
        boolean success = job1.waitForCompletion(true);
        if (!success) {
            System.exit(1);
        }

        // 第二阶段作业：取Top5
        Job job2 = Job.getInstance(conf, "JobCitySalaryTop5");
        job2.setJarByClass(JobCitySalaryTop.class);
        
        job2.setMapperClass(Top5Mapper.class);
        job2.setReducerClass(Top5Reducer.class);
        
        job2.setMapOutputKeyClass(Text.class);
        job2.setMapOutputValueClass(Text.class);
        
        job2.setOutputKeyClass(NullWritable.class);
        job2.setOutputValueClass(Text.class);
        
        FileInputFormat.addInputPath(job2, new Path(otherArgs[1]));
        FileOutputFormat.setOutputPath(job2, new Path(otherArgs[2]));
        
        System.exit(job2.waitForCompletion(true) ? 0 : 1);
    }
}
```

![[image-20250625230541767.png]]

![[image-20250625230555621.png]]

```shell
hdfs dfs -rm -r /data/mr/JobCitySalaryTop
hadoop jar JobCitySalaryTop.jar /data/recruit /data/temp/JobCitySalaryTop /data/mr/JobCitySalaryTop
hdfs dfs -ls /data/mr/JobCitySalaryTop
hdfs dfs -cat /data/mr/JobCitySalaryTop/part-r-00000
```

![[image-20250625230451123.png]]

#### 3.1.4 筛选各城市薪资高于 10k 的职位数

分析的目的：衡量各城市高薪岗位的供给情况，反映城市经济活力和行业薪资水平，帮助求职者判断高薪机会分布，同时为企业制定薪酬策略提供参考。

```java
package xyz.hiubo.bigdata;  
  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.LongWritable;  
import org.apache.hadoop.io.Text;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class CityHighSalaryJobsCount {  
  
    // Mapper：筛选薪资>10k的记录，并按城市分组  
    public static class HighSalaryMapper extends Mapper<LongWritable, Text, Text, LongWritable> {  
        private Text city = new Text();  
        private final LongWritable one = new LongWritable(1);  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 4) { // 确保有薪资字段  
                try {  
                    String cityName = fields[1];  // 城市在第二列  
                    double salary = Double.parseDouble(fields[3]);  // 薪资在第四列  
                    // 筛选薪资>10k的记录  
                    if (salary > 10.0) {  
                        city.set(cityName);  
                        context.write(city, one);  
                    }  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Reducer：统计每个城市的符合条件职位数  
    public static class HighSalaryReducer extends Reducer<Text, LongWritable, Text, LongWritable> {  
        private LongWritable result = new LongWritable();  
  
        @Override  
        protected void reduce(Text key, Iterable<LongWritable> values, Context context) throws IOException, InterruptedException {  
            long count = 0;  
            for (LongWritable val : values) {  
                count += val.get();  
            }  
            result.set(count);  
            context.write(key, result);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: CityHighSalaryJobsCount <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "City High Salary Jobs Count");  
        job.setJarByClass(CityHighSalaryJobsCount.class);  
          
        job.setMapperClass(HighSalaryMapper.class);  
        job.setReducerClass(HighSalaryReducer.class);  
          
        job.setMapOutputKeyClass(Text.class);  
        job.setMapOutputValueClass(LongWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(LongWritable.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

![[image-20250625231431977.png]]

![[image-20250625231442455.png]]

```shell
hdfs dfs -rm -r /data/mr/CityHighSalaryJobsCount
hadoop jar CityHighSalaryJobsCount.jar /data/recruit /data/mr/CityHighSalaryJobsCount
hdfs dfs -ls /data/mr/CityHighSalaryJobsCount
hdfs dfs -cat /data/mr/CityHighSalaryJobsCount/part-r-00000
```

![[image-20250625231454624.png]]

#### 3.1.5 统计不同岗位不同学历不同经验的平均薪资

分析的目的：量化学历、经验对薪资的影响，帮助求职者规划职业发展路径（如是否提升学历或积累经验），同时为企业优化薪酬体系提供数据支撑。

```java
package xyz.hiubo.bigdata;  
  
import java.io.DataInput;  
import java.io.DataOutput;  
import java.io.IOException;  
import org.apache.hadoop.conf.Configuration;  
import org.apache.hadoop.fs.Path;  
import org.apache.hadoop.io.*;  
import org.apache.hadoop.mapreduce.Job;  
import org.apache.hadoop.mapreduce.Mapper;  
import org.apache.hadoop.mapreduce.Reducer;  
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;  
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;  
import org.apache.hadoop.util.GenericOptionsParser;  
  
public class JobEducationExperienceSalaryAvg {  
  
    // 自定义组合键：岗位、学历和经验  
    public static class JobEducationExperienceKey implements WritableComparable<JobEducationExperienceKey> {  
        private Text job;  
        private Text education;  
        private Text experience;  
  
        public JobEducationExperienceKey() {  
            this.job = new Text();  
            this.education = new Text();  
            this.experience = new Text();  
        }  
  
        public JobEducationExperienceKey(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public void set(Text job, Text education, Text experience) {  
            this.job = job;  
            this.education = education;  
            this.experience = experience;  
        }  
  
        public Text getJob() {  
            return job;  
        }  
  
        public Text getEducation() {  
            return education;  
        }  
  
        public Text getExperience() {  
            return experience;  
        }  
  
        @Override  
        public void write(DataOutput out) throws IOException {  
            job.write(out);  
            education.write(out);  
            experience.write(out);  
        }  
  
        @Override  
        public void readFields(DataInput in) throws IOException {  
            job.readFields(in);  
            education.readFields(in);  
            experience.readFields(in);  
        }  
  
        @Override  
        public int compareTo(JobEducationExperienceKey other) {  
            int jobCompare = job.compareTo(other.job);  
            if (jobCompare != 0) {  
                return jobCompare;  
            }  
              
            int educationCompare = education.compareTo(other.education);  
            if (educationCompare != 0) {  
                return educationCompare;  
            }  
              
            return experience.compareTo(other.experience);  
        }  
  
        @Override  
        public int hashCode() {  
            return job.hashCode() * 163 * 163 + education.hashCode() * 163 + experience.hashCode();  
        }  
  
        @Override  
        public boolean equals(Object o) {  
            if (this == o) return true;  
            if (o == null || getClass() != o.getClass()) return false;  
            JobEducationExperienceKey that = (JobEducationExperienceKey) o;  
            return job.equals(that.job) &&   
                   education.equals(that.education) &&   
                   experience.equals(that.experience);  
        }  
  
        @Override  
        public String toString() {  
            return job + "\t" + education + "\t" + experience;  
        }  
    }  
  
    // Mapper：提取岗位、学历、经验和薪资信息  
    public static class SalaryMapper extends Mapper<LongWritable, Text, JobEducationExperienceKey, DoubleWritable> {  
        private JobEducationExperienceKey key = new JobEducationExperienceKey();  
        private DoubleWritable salary = new DoubleWritable();  
  
        @Override  
        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {  
            // 跳过表头行  
            if (key.get() == 0) {  
                return;  
            }  
              
            String[] fields = value.toString().split(",");  
            if (fields.length >= 5) { // 确保有足够的字段  
                try {  
                    String job = fields[2];          // 岗位在第三列  
                    String education = fields[5];    // 学历在第六列  
                    String experience = fields[4];   // 经验在第五列  
                    double salaryValue = Double.parseDouble(fields[3]); // 薪资在第四列  
                    this.key.set(new Text(job), new Text(education), new Text(experience));  
                    salary.set(salaryValue);  
                    context.write(this.key, salary);  
                } catch (NumberFormatException e) {  
                    System.err.println("Invalid salary value: " + value.toString());  
                }  
            }  
        }  
    }  
  
    // Combiner：在Map端进行局部聚合，减少数据传输  
    public static class SalaryCombiner extends Reducer<JobEducationExperienceKey, DoubleWritable, JobEducationExperienceKey, DoubleWritable> {  
        private DoubleWritable result = new DoubleWritable();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            result.set(sum / count);  
            context.write(key, result);  
        }  
    }  
  
    // Reducer：计算平均薪资  
    public static class SalaryReducer extends Reducer<JobEducationExperienceKey, DoubleWritable, Text, Text> {  
        private Text outputKey = new Text();  
        private Text outputValue = new Text();  
  
        @Override  
        protected void reduce(JobEducationExperienceKey key, Iterable<DoubleWritable> values, Context context) throws IOException, InterruptedException {  
            double sum = 0;  
            int count = 0;  
            for (DoubleWritable val : values) {  
                sum += val.get();  
                count++;  
            }  
            double avgSalary = sum / count;  
              
            outputKey.set(key.getJob() + "\t" + key.getEducation() + "\t" + key.getExperience());  
            outputValue.set(String.format("%.2f", avgSalary));  
            context.write(outputKey, outputValue);  
        }  
    }  
  
    public static void main(String[] args) throws Exception {  
        Configuration conf = new Configuration();  
        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();  
          
        if (otherArgs.length != 2) {  
            System.err.println("Usage: JobEducationExperienceSalaryAvg <input path> <output path>");  
            System.exit(2);  
        }  
  
        Job job = Job.getInstance(conf, "Job Education Experience Salary Average");  
        job.setJarByClass(JobEducationExperienceSalaryAvg.class);  
          
        job.setMapperClass(SalaryMapper.class);  
        job.setCombinerClass(SalaryCombiner.class);  
        job.setReducerClass(SalaryReducer.class);  
          
        job.setMapOutputKeyClass(JobEducationExperienceKey.class);  
        job.setMapOutputValueClass(DoubleWritable.class);  
          
        job.setOutputKeyClass(Text.class);  
        job.setOutputValueClass(Text.class);  
          
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));  
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));  
          
        System.exit(job.waitForCompletion(true) ? 0 : 1);  
    }  
}
```

![[image-20250625232401879.png]]

![[image-20250625232420680.png]]

```shell
hdfs dfs -rm -r /data/mr/JobEducationExperienceSalaryAvg
hadoop jar JobEducationExperienceSalaryAvg.jar /data/recruit /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -ls /data/mr/JobEducationExperienceSalaryAvg
hdfs dfs -cat /data/mr/JobEducationExperienceSalaryAvg/part-r-00000
```

![[image-20250625232458992.png]]

### 3.2 使用 Hive 数据分析

启动 Hive
```shell
hive
```

![[image-20250625234158949.png]]

创建 Hive 外部表并验证
```sql
CREATE EXTERNAL TABLE IF NOT EXISTS recruit (
  id INT,
  city STRING,
  position STRING,
  salary DOUBLE,
  experience DOUBLE,
  education STRING,
  company_name STRING,
  company_type STRING,
  company_size INT,
  create_time STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/data/recruit/'
TBLPROPERTIES ("skip.header.line.count"="1");

-- 验证
SELECT * FROM recruit LIMIT 5;
```

![[image-20250625234413525.png]]

#### 3.2.1 查询薪资最高的 10 个职位

分析的目的：识别行业内的顶尖高薪岗位，反映市场稀缺技能或高价值岗位，帮助求职者瞄准高薪方向，同时为企业调整人才吸引策略提供依据。

```sql
SELECT position, salary, company_name, city
FROM recruit
ORDER BY salary DESC
LIMIT 10;
```

![[image-20250625234632516.png]]

#### 3.2.2 统计各城市各岗位的平均薪资

分析的目的：构建城市-岗位薪资矩阵，帮助求职者对比不同地区的薪资差异，同时为企业制定区域差异化薪酬政策提供数据支持。

```sql
SELECT 
  position AS position_name,
  city AS city_name,
  percentile_approx(salary, 0.5) AS median_salary
FROM recruit
WHERE salary IS NOT NULL
GROUP BY position, city
ORDER BY position_name, median_salary DESC;
```

![[image-20250626161526222.png]]

![[image-20250626161606697.png]]

#### 3.2.3 计算不同学历的平均薪资

分析的目的：量化学历对薪资的影响程度，帮助求职者评估学历提升的投资回报率，同时为企业优化薪酬结构（如学历补贴政策）提供参考。

```SQL
SELECT education,AVG(salary) AS avg_salary
	FROM recruit
	GROUP BY education
	ORDER BY avg salary DESC;
```

![[image-20250626161651352.png]]

![[image-20250626161707064.png]]

#### 3.2.4 按月份统计岗位数量趋势

分析的目的：分析招聘市场的季节性波动，识别招聘旺季和淡季，帮助求职者把握最佳求职时机，同时为企业优化招聘节奏提供依据。

```sql
SELECT 
    SUBSTR(create_time, 1, 7) AS month,
    COUNT(*) AS post_count
FROM recruit
GROUP BY SUBSTR(create_time, 1, 7)
ORDER BY month;
```

![[image-20250626161042697.png]]

![[image-20250626161059976.png]]

#### 3.2.5 统计不同公司类型的职位分布

分析的目的：分析不同类型企业（如国企、外企、创业公司）的招聘偏好，帮助求职者选择适合的企业类型，同时为企业研究市场竞争格局提供数据支持。

```sql
SELECT 
    company_type,
    COUNT(*) AS total_positions,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) AS percentage
FROM recruit
GROUP BY company_type
ORDER BY total_positions DESC;
```

![[image-20250626161222800.png]]

![[image-20250626161242867.png]]

### 3.3  Flink 批处理 数据分析

#### 3.3.1 统计不同岗位的分布情况

分析的目的：了解市场上各类岗位的需求比例，反映行业技术趋势（如开发、测试、数据分析等岗位的供需情况），帮助求职者选择热门方向，同时为企业调整人才结构提供参考。

```Java
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.ExecutionEnvironment;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.api.java.utils.ParameterTool;

public class PositionDistributionJob {
    public static void main(String[] args) throws Exception {
        final ParameterTool params = ParameterTool.fromArgs(args);
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
        env.getConfig().setGlobalJobParameters(params);

        DataSet<String> csvInput = env.readTextFile(params.get("input", "/home/hiubo/data/recruit/recruit.csv"));

        DataSet<Tuple2<String, Integer>> positionCounts = csvInput
                .map(new PositionExtractor())
                .filter(record -> !record.f0.equals("岗位"))
                .groupBy(0)
                .sum(1);

        positionCounts.print();
    }

    public static final class PositionExtractor implements MapFunction<String, Tuple2<String, Integer>> {
        @Override
        public Tuple2<String, Integer> map(String line) throws Exception {
            String[] tokens = line.split(",");
            return new Tuple2<>(tokens[2], 1);
        }
    }
}      
```

```Shell
flink run --class xyz.hiubo.bigdata.PositionDistributionJob /home/hiubo/developer/project/code/bigdata/flink-batch1/target/flink-batch1-1.0.jar
```

![[Pasted image 20250628200932.png]]


#### 3.3.2 分析公司规模分布

分析的目的：统计不同规模企业（如小型、中型、大型公司）的招聘情况，帮助求职者了解就业市场结构，同时为企业研究行业竞争格局提供数据支持。

```Java
import org.apache.flink.api.common.functions.MapFunction;  
import org.apache.flink.api.java.DataSet;  
import org.apache.flink.api.java.ExecutionEnvironment;  
import org.apache.flink.api.java.tuple.Tuple2;  
import org.apache.flink.api.java.utils.ParameterTool;  
  
public class CompanySizeAnalysisJob {  
    public static void main(String[] args) throws Exception {  
        final ParameterTool params = ParameterTool.fromArgs(args);  
        final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();  
        env.getConfig().setGlobalJobParameters(params);  
  
        DataSet<String> csvInput = env.readTextFile(params.get("input", "/home/hiubo/data/recruit/recruit.csv"));  
  
        DataSet<Tuple2<String, Integer>> companySizeCounts = csvInput  
                .map(new CompanySizeExtractor())  
                .filter(record -> !record.f0.equals("公司规模"))  
                .groupBy(0)  
                .sum(1);  
  
        companySizeCounts.print();  
    }  
  
    public static final class CompanySizeExtractor implements MapFunction<String, Tuple2<String, Integer>> {  
        @Override  
        public Tuple2<String, Integer> map(String line) throws Exception {  
            String[] tokens = line.split(",");  
            return new Tuple2<>(tokens[8], 1);  
        }  
    }  
} 
```

```Shell
flink run --class xyz.hiubo.bigdata.CompanySizeAnalysisJob /home/hiubo/developer/project/code/bigdata/flink-batch2/target/flink-batch2-1.0.jar
```

![[Pasted image 20250627225134.png]]

## 4. 流数据处理

### 4.1 Flink 流数据处理 1 (文件)

#### 4.1.1 实时统计城市岗位数量

分析的目的：动态监测各城市的招聘需求变化，帮助求职者快速捕捉最新机会，同时为企业实时调整招聘策略（如区域扩张或收缩）提供决策依据。

```Java
package xyz.hiubo.bigdata;  
  
import org.apache.flink.api.common.eventtime.WatermarkStrategy;  
import org.apache.flink.api.common.functions.FlatMapFunction;  
import org.apache.flink.api.java.tuple.Tuple3;  
import org.apache.flink.configuration.Configuration;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.connector.file.src.FileSource;  
import org.apache.flink.connector.file.src.reader.TextLineInputFormat;  
import org.apache.flink.core.fs.Path;  
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;  
import org.apache.flink.streaming.api.windowing.time.Time;  
import org.apache.flink.util.Collector;  
  
import java.time.Duration;  
  
public class CityJobCount {  
  
    public static void main(String[] args) throws Exception {  
  
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
        env.setParallelism(1);  
  
        // 啟用 checkpoint 以支持持續監聽新文件  
        env.enableCheckpointing(10000);  
  
        // 使用 TextLineInputFormat 按行讀取文本文件  
        FileSource<String> fileSource = FileSource.forRecordStreamFormat(  
                        new TextLineInputFormat(),  
                        new Path("/home/hiubo/data/recruit/stream1")  
                )  
                .monitorContinuously(Duration.ofSeconds(5))  
                .build();  
  
        DataStream<String> lines = env.fromSource(fileSource, WatermarkStrategy.noWatermarks(), "File Source");  
  
        // 解析每行為 (city, job, count)        DataStream<Tuple3<String, String, Integer>> parsed = lines.flatMap(  
                new FlatMapFunction<String, Tuple3<String, String, Integer>>() {  
                    @Override  
                    public void flatMap(String line, Collector<Tuple3<String, String, Integer>> out) {  
                        String[] parts = line.split(",");  
                        if (parts.length == 2) {  
                            String city = parts[0].trim();  
                            String job = parts[1].trim();  
                            out.collect(Tuple3.of(city, job, 1));  
                        }  
                    }  
                }  
        );  
  
        // 按照 (city, job) 分組並統計數量  
        parsed.keyBy(value -> value.f0 + ":" + value.f1)  
                .sum(2)  
                .print();  
  
        env.execute("City Job Count Streaming");  
    }  
}
```

![[Pasted image 20250628174143.png]]

![[Pasted image 20250628174208.png]]

![[Pasted image 20250628174227.png]]


### 4.2 Flink 流数据处理 2 (Kafka)

启动 Kafka 服务
```Shell
kafka-server-start.sh $KAFKA_HOME/config/server.properties
```
![[Pasted image 20250628220443.png]]

创建 Topic 
```Shell
kafka-topics.sh --create --topic industry-job --bootstrap-server localhost:9092 --partitions 1 -replication-factor 1
```
![[Pasted image 20250628205619.png]]

## 5. FastAPI 与 ECharts 数据可视化

### 5.1 开发环境

#### 5.1.1 FastAPI 开发环境

Miniconda 3, Python 3.12, FastAPI 0.115.14, Echarts5

#### 5.1.2 本部分功能

FastAPI 是一个现代化、高性能的 Python Web 框架，专为构建高效、易维护的 API 而设计。框架充分利用 Python 的类型提示系统，结合 Pydantic 实现强大的数据验证和自动序列化，在编写代码时就能捕获大量潜在错误。开发过程中，内置的热重载功能通过简单的 `--reload` 参数实现实时更新，配合详尽的错误提示和调试支持，极大提升了开发效率。

后端需要为前端 Echarts 的可视化提供获取数据的功能接口有 : 
1. 统计每个学历要求出现的次数
2. 统计各岗位在各城市的招聘数量 top5
3. 统计各岗位在各城市的平均薪资 top5
4. 筛选各城市薪资高于 10k 的职位数
5. 统计不同岗位不同学历不同经验的平均薪资

### 5.2 数据可视化 1

#### 5.2.1 各学历要求的占比

通过饼图可视化“学历词频”可以清晰展示不同教育程度在招聘需求中的占比，帮助快速理解各类学历的分布情况。

#### 5.2.2 项目结构

`/app` 存放后端接口与前端页面的所有代码
	`main.py` 建立后端应用, 编写数据接口与解决跨域问题。
	`/templates` 放前端Echarts可视化的页面。
		education_word_count.html

#### 5.2.3 HTML 模板

```HTML
<!DOCTYPE html>  
<html style="height: 100%">  
<head>  
    <meta charset="utf-8">  
    <title>学历词频统计</title>  
    <script src="https://cdn.jsdelivr.net/npm/echarts@5/dist/echarts.min.js"></script>  
    <style>        html, body {  
            height: 100%;  
            margin: 0;  
            display: flex;  
            justify-content: center;  
            align-items: center;  
        }  
        #main {  
            width: 800px;  
            height: 600px;  
        }  
    </style>  
</head>  
<body>  
<div id="main"></div>  
<script type="text/javascript">  
    var chartDom = document.getElementById('main');  
    var myChart = echarts.init(chartDom);  
    var option;  
  
    fetch('http://localhost:8000/EducationWordCount')  
        .then(response => response.json())  
        .then(data => {  
            option = {  
                title: {  
                    text: '学历词频统计',  
                    left: 'center'  
                },  
                tooltip: {  
                    trigger: 'item'  
                },  
                legend: {  
                    orient: 'vertical',  
                    left: 'left'  
                },  
                series: [{  
                    name: '学历',  
                    type: 'pie',  
                    radius: '50%',  
                    data: data.map(item => ({value: item.count, name: item.education})),  
                    label: {  
                        formatter: ({name, value, percent}) => `${name}: ${value} (${percent.toFixed(1)}%)`  
                    },  
                    emphasis: {  
                        itemStyle: {  
                            shadowBlur: 10,  
                            shadowOffsetX: 0,  
                            shadowColor: 'rgba(0, 0, 0, 0.5)'  
                        }  
                    }  
                }]  
            };  
  
            option && myChart.setOption(option);  
        });  
</script>  
</body>  
</html>
```

#### 5.2.4 Python 后端代码

```python
from fastapi import FastAPI  
from fastapi.responses import JSONResponse  
from fastapi.responses import FileResponse  
import os  
from fastapi.middleware.cors import CORSMiddleware  
app = FastAPI()  
  
# 允许来自所有来源的跨域请求  
app.add_middleware(  
    CORSMiddleware,  
    allow_origins=["*"],  # 允许所有来源  
    allow_credentials=True,  
    allow_methods=["*"],    # 允许所有方法  
    allow_headers=["*"],   # 允许所有头部  
)  
  
def read_file_data(file_path: str):  
    if os.path.exists(file_path):  
        with open(file_path, 'r') as file:  
            lines = file.readlines()  
            return [line.strip() for line in lines]  
    return []  
  
@app.get("/EducationWordCount")  
def read_education_word_count():  
    file_path = "mr/EducationWordCount/part-r-00000"  
    data = read_file_data(file_path)  
    # 解析: 第一列是学历, 第二列是数量  
    data = [{"education": line.split("\t")[0], "count": int(line.split("\t")[1])} for line in data]  
    return JSONResponse(content=data)

if __name__ == "__main__":  
    import uvicorn  
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### 5.2.5 运行与页面效果展示

![[Pasted image 20250628235834.png]]

![[Pasted image 20250629000309.png]]

### 5.3 各岗位在各城市的招聘数量 top5

#### 5.3.1 功能说明

通过分组柱状图或堆叠柱状图展示不同城市中各岗位的数量差异，便于直观比较城市之间的岗位分布情况
比较不同城市中各岗位的数量差异
- X轴：城市名称
- 图例：岗位类型
- Y轴：岗位数量
用于帮助用户快速识别哪些城市在特定岗位上有更高的招聘需求

#### 5.3.2 运行与页面效果展示

![[Pasted image 20250629003715.png]]

### 5.4 各岗位在各城市的平均薪资 top5

#### 5.4.1 功能说明

使用折线图展示不同岗位在各个城市的平均薪资变化趋势，便于观察岗位薪资在不同城市间的波动情况。
- X轴：城市
- Y轴：平均薪资
- 每条折线：代表一个岗位的薪资分布
用于帮助用户直观分析不同岗位在各地的薪资走向和差异。

#### 5.4.2 运行与页面效果展示

![[Pasted image 20250629004936.png]]

### 5.5 各城市薪资高于 10k 的职位数

#### 5.5.1 功能说明

使用地图可视化展示各城市高薪岗位数量的地理分布，颜色深浅反映岗位数量多少，更直观地呈现地域差异。
- 地图区域：城市或省份
- 颜色映射：高薪岗位数量
- 用途：快速识别高薪岗位密集区域
帮助用户从空间维度理解就业市场中薪资分布的地域特征。

#### 5.5.2 运行与页面效果展示

![[Pasted image 20250629012601.png]]

### 5.6 不同岗位不同学历不同经验的平均薪资

#### 5.6.1 功能说明

该功能使用热力图展示岗位薪资与学历、经验年限之间的关系，颜色深浅直观反映薪资水平在不同学历与经验组合下的分布情况。
- X轴：经验年限
- Y轴：学历要求
- 颜色强度：代表薪资高低
帮助用户快速识别高薪岗位在学历与经验维度上的分布规律

#### 5.6.2 运行与页面效果展示

![[Pasted image 20250629010532.png]]

#### 4.2.1 实时消费 Kafka 数据并统计行业分布

分析的目的：对最新招聘数据进行实时处理和监控，及时掌握各行业人才需求变化趋势，为行业分析和市场预测提供最及时的 数据支持；确保招聘平台能够第一时间根据行业动态调整推荐策略，满足不同行业的人才招聘需求。


#### 4.2.2 消费者

```Java
package xyz.hiubo.bigdata;  
  
import org.apache.flink.api.common.functions.FlatMapFunction;  
import org.apache.flink.api.common.typeinfo.TypeInformation;  
import org.apache.flink.api.java.tuple.Tuple2;  
import org.apache.flink.streaming.api.datastream.DataStream;  
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;  
import org.apache.flink.streaming.api.windowing.time.Time;  
import org.apache.flink.streaming.connectors.kafka.KafkaDeserializationSchema;  
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;  
import org.apache.flink.util.Collector;  
import org.apache.kafka.clients.consumer.ConsumerRecord;  
import java.nio.charset.StandardCharsets;  
import java.util.Properties;  
  
public class FlinkKafkaJobStats {  
    public static void main(String[] args) throws Exception {  
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();  
  
        Properties properties = new Properties();  
        properties.setProperty("bootstrap.servers", "localhost:9092");  
        properties.setProperty("group.id", "flink-consumer-group");  
        properties.setProperty("auto.offset.reset", "earliest");  
  
        // 自定义 Kafka Deserialization Schema        KafkaDeserializationSchema<String> deserializer = new KafkaDeserializationSchema<String>() {  
            @Override  
            public boolean isEndOfStream(String nextElement) {  
                return false;  
            }  
  
            @Override  
            public String deserialize(ConsumerRecord<byte[], byte[]> record) {  
                if (record.value() == null) {  
                    return null;  
                }  
                return new String(record.value(), StandardCharsets.UTF_8);  
            }  
  
            @Override  
            public TypeInformation<String> getProducedType() {  
                return TypeInformation.of(String.class);  
            }  
        };  
  
        DataStream<String> kafkaStream = env.addSource(  
                new FlinkKafkaConsumer<>("industry-job", deserializer, properties)  
        );  
  
        DataStream<Tuple2<String, Integer>> parsedStream = kafkaStream  
                .flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {  
                    @Override  
                    public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {  
                        String industry = value.trim();  
                        if (!industry.isEmpty()) {  
                            out.collect(Tuple2.of(industry, 1));  
                        }  
                    }  
                });  
  
        parsedStream.keyBy(value -> value.f0)  
                .timeWindow(Time.seconds(5))  
                .sum(1)  
                .print();  
  
        env.execute("Flink Kafka Industry Job Count");  
    }  
}
```

#### 4.2.3 生产者

```Java
package xyz.hiubo.bigdata;  
  
import org.apache.kafka.clients.producer.*;  
import com.opencsv.CSVReader;  
import java.io.FileReader;  
import java.util.Properties;  
import java.util.concurrent.Executors;  
import java.util.concurrent.ScheduledExecutorService;  
import java.util.concurrent.TimeUnit;  
  
public class KafkaCsvProducer {  
  
    private static final String TOPIC = "industry-job";  
    private static final String CSV_FILE_PATH = "/home/hiubo/data/recruit/industry_job.csv";  
  
    public static void main(String[] args) {  
        Properties props = new Properties();  
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");  
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");  
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");  
  
        Producer<String, String> producer = new KafkaProducer<>(props);  
  
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);  
        scheduler.scheduleAtFixedRate(() -> {  
            try (CSVReader reader = new CSVReader(new FileReader(CSV_FILE_PATH))) {  
                int count = 0;  
                String[] nextLine;  
                while ((nextLine = reader.readNext()) != null && count++ < 1000) {  
                    String industry = nextLine[0]; // 第一列是行业  
                    producer.send(new ProducerRecord<>(TOPIC, industry));  
                }  
                System.out.println("Sent 1000 records to Kafka.");  
            } catch (Exception e) {  
                e.printStackTrace();  
            }  
        }, 0, 5, TimeUnit.SECONDS);  
    }  
}
```

![[Pasted image 20250628220809.png|204]]

#### 4.2.4 Flink Consumer 执行结果

![[Pasted image 20250628215935.png|135]]

![[Pasted image 20250628215949.png|160]]

![[Pasted image 20250628215958.png|160]]

## 6. 总结

本项目聚焦于IT行业招聘信息领域展开深度大数据分析，精心构建起一套高度整合的数据分析平台，有机融合了HDFS、MapReduce、Hive、Flink以及FastAPI+ECharts等核心技术组件。具体而言，通过八爪鱼工具从Boss直聘平台成功爬取13198条招聘数据，这些数据涵盖城市、薪资、经验要求、学历要求等18个关键字段。随后借助Python进行数据预处理，完成去除重复行、薪资中值提取、经验要求标准化等操作，为后续分析奠定高质量数据基础。在数据存储环节，利用HDFS分布式文件系统实现海量数据的可靠存储，通过三副本机制保障数据安全性，同时支持多种格式数据的高效读写。 

在数据分析层面，采用分层处理架构：借助MapReduce框架完成岗位数量统计、薪资区间分布、技能词频分析等离线计算任务，例如统计出本科学历需求达9553次，大专为2470次；通过Hive构建数据仓库，结合HQL实现多维度聚合查询，如按月份统计岗位数量趋势、筛选高薪职位等；基于Flink框架实现流批一体计算，运用事件时间语义和滑动窗口技术，实时追踪热门技术趋势变化和区域薪资波动情况，例如对深圳、上海等城市的Java后端岗位薪资进行实时监测。 

最终通过FastAPI构建高性能RESTful API服务，结合ECharts丰富的可视化图表库，实现动态热力图、实时折线图、多维雷达图等多种可视化展示。其中，学历要求分布饼图直观呈现本科占比75.3%、大专占比19.5%的结构特征，城市岗位数量TOP5柱状图清晰显示杭州、深圳等城市的Java后端岗位需求超297个。该项目不仅为IT企业提供精准的人才市场分析，助力企业优化招聘策略与人力资源配置，也为求职者提供学历、经验与薪资关联的量化参考，帮助其规划职业发展路径，充分展现了大数据技术在招聘市场分析中的应用价值，实现了从数据爬取、预处理、存储、分析到可视化的全流程技术整合与业务赋能。

